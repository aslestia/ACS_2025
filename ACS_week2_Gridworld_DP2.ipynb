{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcg9izYuPnJvUKHuAP5Q/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslestia/ACS_2025/blob/main/ACS_week2_Gridworld_DP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoDSiTOaTS1u"
      },
      "outputs": [],
      "source": [
        "# PSEUDOCODE - GridWorld lengkap (sampai DP)\n",
        "#### VALUE ITERATION (VI)\n",
        "def value_iteration(env, eps=1e-6, max_iter=1000):\n",
        "    V = np.zeros(env.nS)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        delta = 0.0\n",
        "        V_new = np.copy(V)\n",
        "\n",
        "        for i in range(env.nS):\n",
        "            s = env.state_from_idx(i)\n",
        "            if env.is_terminal(s):\n",
        "                V_new[i] = 0.0\n",
        "                continue\n",
        "\n",
        "            # Hitung V_new(s) dengan mengambil aksi terbaik (max)\n",
        "            action_values = []\n",
        "            for a in range(env.n_actions):\n",
        "                q_value = 0.0\n",
        "                for p, sn, r in env.transitions(s, a):\n",
        "                    q_value += p * (r + env.gamma * V[env.idx(sn)])\n",
        "                action_values.append(q_value)\n",
        "\n",
        "            V_new[i] = np.max(action_values)\n",
        "            delta = max(delta, abs(V_new[i] - V[i]))\n",
        "\n",
        "        V = V_new\n",
        "        if delta < eps:\n",
        "            break\n",
        "\n",
        "    # Ekstrak kebijakan optimal dari V yang telah konvergen\n",
        "    Pi = np.zeros(env.nS, dtype=int)\n",
        "    for i in range(env.nS):\n",
        "        s = env.state_from_idx(i)\n",
        "        action_values = []\n",
        "        for a in range(env.n_actions):\n",
        "            q_value = 0.0\n",
        "            for p, sn, r in env.transitions(s, a):\n",
        "                q_value += p * (r + env.gamma * V[env.idx(sn)])\n",
        "            action_values.append(q_value)\n",
        "\n",
        "        Pi[i] = np.argmax(action_values)\n",
        "\n",
        "    return Pi, V\n",
        "\n",
        "  #### POLICY ITERATION (PI)\n",
        "  def policy_iteration(env, eps=1e-6, max_iter=1000):\n",
        "    # Langkah 1: Inisialisasi kebijakan acak\n",
        "    Pi = np.random.choice(env.n_actions, env.nS)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        # Langkah 2: Evaluasi kebijakan saat ini (Pi)\n",
        "        V = policy_evaluation(env, Pi, eps)\n",
        "\n",
        "        # Langkah 3: Perbaikan kebijakan (Policy Improvement)\n",
        "        policy_stable = True\n",
        "        for i in range(env.nS):\n",
        "            s = env.state_from_idx(i)\n",
        "            old_action = Pi[i]\n",
        "\n",
        "            # Cari aksi terbaik (greedy) berdasarkan V\n",
        "            action_values = []\n",
        "            for a in range(env.n_actions):\n",
        "                q_value = 0.0\n",
        "                for p, sn, r in env.transitions(s, a):\n",
        "                    q_value += p * (r + env.gamma * V[env.idx(sn)])\n",
        "                action_values.append(q_value)\n",
        "\n",
        "            best_action = np.argmax(action_values)\n",
        "            Pi[i] = best_action\n",
        "\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "\n",
        "        if policy_stable:\n",
        "            return Pi, V # Konvergen, kebijakan optimal ditemukan\n",
        "\n",
        "    return Pi, V\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code TODO — GridWorld lengkap (tugas)\n",
        "#Isi baris bertanda # TODO.\n",
        "\n",
        "import numpy as np, time, matplotlib.pyplot as plt\n",
        "\n",
        "# Parameter mudah diubah\n",
        "H, W = 4, 4\n",
        "START = (0,0)\n",
        "GOALS = {(3,3): +10}\n",
        "TRAPS = {(1,2):-10, (2,1):-10}\n",
        "STEP_REWARD = -1.0\n",
        "GAMMA = 0.95\n",
        "SLIP = 0.0   # 0.1 untuk transisi probabilistik\n",
        "EPS = 1e-6\n",
        "ACTIONS = {0: (-1,0), 1: (0,1), 2: (1,0), 3: (0,-1)}   # U,R,D,L\n",
        "ARROWS  = {0:\"↑\", 1:\"→\", 2:\"↓\", 3:\"←\", -1:\"■\"}\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, H, W, start, goals, traps, step_reward, gamma, slip=0.0):\n",
        "        self.H, self.W = H, W\n",
        "        self.start = start\n",
        "        self.goals = goals\n",
        "        self.traps = traps\n",
        "        self.step_reward = step_reward\n",
        "        self.gamma = gamma\n",
        "        self.slip = slip\n",
        "        self.nS, self.nA = H*W, 4\n",
        "\n",
        "    def idx(self, s): return s[0]*self.W + s[1]\n",
        "    def state_from_idx(self, i): return (i//self.W, i%self.W)\n",
        "    def in_bounds(self, r,c): return 0<=r<self.H and 0<=c<self.W\n",
        "    def is_terminal(self, s): return s in self.goals or s in self.traps\n",
        "\n",
        "    def transitions(self, s, a):\n",
        "        if self.is_terminal(s): return [(1.0, s, 0.0)]\n",
        "        dr, dc = ACTIONS[a]\n",
        "        cand = (s[0]+dr, s[1]+dc)\n",
        "        if not self.in_bounds(*cand): cand = s\n",
        "        left, right = (a-1)%4, (a+1)%4\n",
        "        candL = (s[0]+ACTIONS[left][0], s[1]+ACTIONS[left][1])\n",
        "        candR = (s[0]+ACTIONS[right][0], s[1]+ACTIONS[right][1])\n",
        "        candL = candL if self.in_bounds(*candL) else s\n",
        "        candR = candR if self.in_bounds(*candR) else s\n",
        "        p_main = 1.0 - 2*self.slip\n",
        "        outs = [(p_main, cand), (self.slip, candL), (self.slip, candR)]\n",
        "        ts = []\n",
        "        for p, sn in outs:\n",
        "            if p<=0: continue\n",
        "            r = self.goals.get(sn, self.traps.get(sn, self.step_reward))\n",
        "            ts.append((p, sn, r))\n",
        "        return ts\n",
        "\n",
        "def print_values(env, V):\n",
        "    grid = np.zeros((env.H, env.W))\n",
        "    for i in range(env.nS):\n",
        "        r,c = env.state_from_idx(i); grid[r,c] = V[i]\n",
        "    for r in range(env.H):\n",
        "        print(\" | \".join(f\"{grid[r,c]:4.1f}\" for c in range(env.W)))\n",
        "    print()\n",
        "\n",
        "def print_policy(env, Pi):\n",
        "    for r in range(env.H):\n",
        "        row=[]\n",
        "        for c in range(env.W):\n",
        "            s=(r,c)\n",
        "            row.append(ARROWS[-1] if env.is_terminal(s) else ARROWS[Pi[env.idx(s)]])\n",
        "        print(\" \".join(row))\n",
        "    print()\n",
        "\n",
        "def show_grid(env):\n",
        "    fig, ax = plt.subplots(figsize=(4,4))\n",
        "    for x in range(env.W+1): ax.plot([x,x],[0,env.H],color=\"k\",lw=1)\n",
        "    for y in range(env.H+1): ax.plot([0,env.W],[y,y],color=\"k\",lw=1)\n",
        "    for (r,c),val in env.goals.items():\n",
        "        ax.add_patch(plt.Rectangle((c, r),1,1,color=\"green\",alpha=0.25))\n",
        "        ax.text(c+0.5,r+0.5,f\"GOAL\\n{val:+}\",ha=\"center\",va=\"center\")\n",
        "    for (r,c),val in env.traps.items():\n",
        "        ax.add_patch(plt.Rectangle((c, r),1,1,color=\"red\",alpha=0.25))\n",
        "        ax.text(c+0.5,r+0.5,f\"TRAP\\n{val:+}\",ha=\"center\",va=\"center\")\n",
        "    sr,sc = env.start\n",
        "    ax.text(sc+0.5,sr+0.5,\"START\",color=\"blue\",ha=\"center\",va=\"center\")\n",
        "    ax.set_xlim(0,env.W); ax.set_ylim(env.H,0); ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.set_title(\"GridWorld\"); plt.show()\n",
        "\n",
        "# ----------- TODO 1: Value Iteration -----------\n",
        "def value_iteration(env, eps=1e-6, max_iter=1000):\n",
        "    V = np.zeros(env.nS)\n",
        "    t0 = time.time(); iters = 0\n",
        "    for it in range(max_iter):\n",
        "        iters = it + 1\n",
        "        delta = 0.0\n",
        "        V_new = V.copy()\n",
        "        for i in range(env.nS):\n",
        "            s = env.state_from_idx(i)\n",
        "            if env.is_terminal(s):\n",
        "                V_new[i] = 0.0\n",
        "                continue\n",
        "\n",
        "            # TODO: Hitung nilai Bellman optimal untuk state s\n",
        "            # Rumus: V(s) = max_a [ R(s,a) + gamma * sum(p(s'|s,a)*V(s')) ]\n",
        "            q_values = np.zeros(env.nA)\n",
        "            for a in range(env.nA):\n",
        "                for p, sn, r in env.transitions(s, a):\n",
        "                    q_values[a] += p * (r + env.gamma * V[env.idx(sn)])\n",
        "\n",
        "            V_new[i] = np.max(q_values)\n",
        "            delta = max(delta, abs(V_new[i] - V[i]))\n",
        "\n",
        "        V = V_new\n",
        "        if delta < eps:\n",
        "            break\n",
        "\n",
        "    # Ekstrak kebijakan greedy dari V\n",
        "    Pi=np.full(env.nS,-1,int)\n",
        "    for i in range(env.nS):\n",
        "        s=env.state_from_idx(i)\n",
        "        if env.is_terminal(s): continue\n",
        "        qbest,a_best=-1e18,0\n",
        "        for a in range(4):\n",
        "            q=0.0\n",
        "            for p,sn,r in env.transitions(s,a):\n",
        "                q += p*(r + env.gamma*V[env.idx(sn)])\n",
        "            if q>qbest: qbest,a_best=q,a\n",
        "        Pi[i]=a_best\n",
        "    return V, Pi, iters, time.time()-t0\n",
        "\n",
        "# ----------- TODO 2: Policy Evaluation -----------\n",
        "def policy_evaluation(env, Pi, eps=1e-6, max_iter=1000):\n",
        "    V = np.zeros(env.nS)\n",
        "    for _ in range(max_iter):\n",
        "        delta = 0.0\n",
        "        for i in range(env.nS):\n",
        "            s = env.state_from_idx(i)\n",
        "            if env.is_terminal(s):\n",
        "                newv = 0.0\n",
        "            else:\n",
        "                a = Pi[i]\n",
        "                newv = 0.0\n",
        "                # TODO: Evaluasi nilai state berdasarkan kebijakan Pi\n",
        "                # Rumus: V(s) = R(s,a) + gamma * sum(p(s'|s,a)*V(s'))\n",
        "                for p, sn, r in env.transitions(s, a):\n",
        "                    newv += p * (r + env.gamma * V[env.idx(sn)])\n",
        "\n",
        "            delta = max(delta, abs(newv - V[i]))\n",
        "            V[i] = newv\n",
        "\n",
        "        if delta < eps:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "# ----------- TODO 3: Policy Improvement -----------\n",
        "def policy_improvement(env, V):\n",
        "    Pi = np.full(env.nS, -1, int)\n",
        "    stable = True\n",
        "    for i in range(env.nS):\n",
        "        s = env.state_from_idx(i)\n",
        "        if env.is_terminal(s):\n",
        "            continue\n",
        "\n",
        "        old_a = Pi[i]\n",
        "\n",
        "        # TODO: Pilih aksi terbaik (greedy) berdasar V\n",
        "        q_values = np.zeros(env.nA)\n",
        "        for a in range(env.nA):\n",
        "            for p, sn, r in env.transitions(s, a):\n",
        "                q_values[a] += p * (r + env.gamma * V[env.idx(sn)])\n",
        "\n",
        "        best_a = np.argmax(q_values)\n",
        "\n",
        "        Pi[i] = best_a\n",
        "        if old_a != best_a:\n",
        "            stable = False\n",
        "\n",
        "    return Pi, stable\n",
        "\n",
        "def policy_iteration(env, eps=1e-6, max_iter=100):\n",
        "    Pi = np.zeros(env.nS, dtype=int)\n",
        "    for i in range(env.nS):\n",
        "        if env.is_terminal(env.state_from_idx(i)): Pi[i] = -1\n",
        "\n",
        "    t0 = time.time()\n",
        "    iters = 0\n",
        "    for k in range(max_iter):\n",
        "        iters = k + 1\n",
        "        V = policy_evaluation(env, Pi, eps=eps)\n",
        "        newPi, stable = policy_improvement(env, V)\n",
        "        if stable:\n",
        "            return V, newPi, iters, time.time() - t0\n",
        "        Pi = newPi\n",
        "\n",
        "    return V, Pi, iters, time.time() - t0\n",
        "\n",
        "env = GridWorld(H,W,START,GOALS,TRAPS,STEP_REWARD,GAMMA,SLIP)\n",
        "show_grid(env)\n",
        "\n",
        "# Jalankan dan cetak hasil Value Iteration\n",
        "V_vi, Pi_vi, it_vi, t_vi = value_iteration(env, eps=EPS)\n",
        "print(\"Value Iteration — Values:\"); print_values(env, V_vi)\n",
        "print(\"Value Iteration — Policy:\"); print_policy(env, Pi_vi)\n",
        "print(f\"Iterasi: {it_vi} | Waktu: {t_vi:.4f}s\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Jalankan dan cetak hasil Policy Iteration\n",
        "V_pi, Pi_pi, it_pi, t_pi = policy_iteration(env, eps=EPS)\n",
        "print(\"Policy Iteration — Values:\"); print_values(env, V_pi)\n",
        "print(\"Policy Iteration — Policy:\"); print_policy(env, Pi_pi)\n",
        "print(f\"Iterasi: {it_pi} | Waktu: {t_pi:.4f}s\")\n",
        "\n",
        "# Fungsi untuk menjalankan skenario\n",
        "def run_scenario(gamma=0.95, slip=0.0, step=-1.0, goals=None, traps=None, title=\"\"):\n",
        "    env = GridWorld(H,W,START, goals or GOALS, traps or TRAPS, step, gamma, slip)\n",
        "    V_vi, Pi_vi, it_vi, _ = value_iteration(env, eps=EPS)\n",
        "    V_pi, Pi_pi, it_pi, _ = policy_iteration(env, eps=EPS)\n",
        "    print(f\"== {title} ==\")\n",
        "    print(f\"gamma={gamma}, slip={slip}, step={step}\")\n",
        "    print(\"VI iters:\", it_vi, \"| PI iters:\", it_pi)\n",
        "    return (env, V_vi, Pi_vi, V_pi, Pi_pi)\n",
        "\n",
        "# Contoh:\n",
        "_ = run_scenario(gamma=0.9, title=\"Skenario A (gamma 0.9)\")\n",
        "_ = run_scenario(slip=0.1, title=\"Skenario B (slip 0.1)\")\n",
        "_ = run_scenario(traps={(1,2):-5,(2,1):-5}, title=\"Skenario C (trap -5)\")"
      ],
      "metadata": {
        "id": "6lhQCuAtTYH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}