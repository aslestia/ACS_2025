{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslestia/ACS_2025/blob/main/TD_CartPole_SARSA_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpKrnf6guYTY"
      },
      "source": [
        "# TD Learning on CartPole (SARSA & Q-learning)\n",
        "\n",
        "Notebook ini menerapkan **Temporal Difference (TD) Learning** pada lingkungan **CartPole-v1** (Gymnasium),\n",
        "dengan pendekatan **discretization** (mendisretkan state kontinu menjadi indeks diskrit) agar bisa memakai tabel-Q.\n",
        "\n",
        "Algoritma:\n",
        "- **SARSA (on-policy)**\n",
        "- **Q-learning (off-policy)**\n",
        "\n",
        "Kita sertakan komentar rinci agar mudah mengikuti alur kode."
      ],
      "id": "QpKrnf6guYTY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLPLDhxMuYTe"
      },
      "source": [
        "## 0) Setup & Imports\n",
        "Jika perlu, jalankan `pip install` untuk memasang Gymnasium.\n",
        "Anda bisa melewati sel `pip` jika sudah terpasang."
      ],
      "id": "rLPLDhxMuYTe"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oR1IET7ouYTg"
      },
      "outputs": [],
      "source": [
        "# Jika perlu, hapus komentar berikut:\n",
        "# !pip install gymnasium==0.29.1 numpy matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gymnasium as gym"
      ],
      "id": "oR1IET7ouYTg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpvcWOnRuYTi"
      },
      "source": [
        "## 1) Environment: CartPole-v1\n",
        "CartPole memiliki state kontinyu 4 dimensi:\n",
        "1) posisi cart, 2) kecepatan cart, 3) sudut tiang, 4) kecepatan sudut tiang.\n",
        "\n",
        "Aksi (diskrit): 0 = dorong ke kiri, 1 = dorong ke kanan.\n",
        "\n",
        "Episode berakhir jika tiang jatuh melewati batas sudut atau cart melewati batas posisi. Reward = 1 per langkah. Maksimal langkah biasanya 500."
      ],
      "id": "YpvcWOnRuYTi"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0KnnmawxuYTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6232d9ef-1f88-421f-9cac-ded7ce673e07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(2)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "n_actions = env.action_space.n\n",
        "n_actions"
      ],
      "id": "0KnnmawxuYTj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufTDLSHiuYTl"
      },
      "source": [
        "## 2) Discretization (State Binning)\n",
        "Karena state kontinu, kita perlu memetakannya ke indeks diskrit agar Q-table bisa dipakai.\n",
        "\n",
        "Strategi sederhana:\n",
        "- Tetapkan rentang (min, max) untuk tiap dimensi (clipping agar stabil).\n",
        "- Bagi tiap dimensi menjadi sejumlah bin (contoh: [6, 6, 12, 12]).\n",
        "- Gunakan `np.digitize` untuk memetakan nilai ke indeks bin.\n",
        "- Gabungkan ke satu indeks tunggal dengan radix/basis (atau simpan sebagai tuple indeks)."
      ],
      "id": "ufTDLSHiuYTl"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1yfa82mOuYTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b2db38-d982-4b05-99ac-4ece5dfc9eca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 6, 12, 12, np.int64(2))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Konfigurasi bin untuk tiap dimensi state\n",
        "NUM_BINS = (6, 6, 12, 12)  # cart_pos, cart_vel, pole_angle, pole_vel\n",
        "\n",
        "# Batasan (clipping) untuk tiap dimensi agar diskretisasi tidak meledak\n",
        "STATE_BOUNDS = np.array([\n",
        "    [-2.4, 2.4],        # cart position (env termination bound)\n",
        "    [-3.0, 3.0],        # cart velocity (dibatasi agar masuk akal)\n",
        "    [-0.2095, 0.2095],  # pole angle (~12 degrees)\n",
        "    [-3.5, 3.5]         # pole velocity at tip (dibatasi)\n",
        "], dtype=float)\n",
        "\n",
        "def create_bins(low, high, bins):\n",
        "    \"\"\"Buat batas bin (tanpa termasuk -inf/inf) untuk np.digitize.\"\"\"\n",
        "    return np.linspace(low, high, bins - 1)\n",
        "\n",
        "# Precompute batas bin untuk tiap dimensi\n",
        "BIN_EDGES = [create_bins(STATE_BOUNDS[i,0], STATE_BOUNDS[i,1], NUM_BINS[i]) for i in range(4)]\n",
        "\n",
        "def discretize_state(state):\n",
        "    \"\"\"Map state kontinu -> tuple indeks diskrit (i0,i1,i2,i3).\"\"\"\n",
        "    s = np.array(state, dtype=float)\n",
        "    # Clip agar dalam batas\n",
        "    s = np.clip(s, STATE_BOUNDS[:,0], STATE_BOUNDS[:,1])\n",
        "    idxs = [int(np.digitize(s[i], BIN_EDGES[i])) for i in range(4)]\n",
        "    # Pastikan indeks dalam [0, bins-1]\n",
        "    idxs = [min(NUM_BINS[i]-1, max(0, idxs[i])) for i in range(4)]\n",
        "    return tuple(idxs)\n",
        "\n",
        "def q_shape():\n",
        "    return (*NUM_BINS, n_actions)  # contoh: (6,6,12,12,2)\n",
        "\n",
        "q_shape()"
      ],
      "id": "1yfa82mOuYTm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSw9Cc16uYTn"
      },
      "source": [
        "## 3) Utilitas: epsilon-greedy, evaluasi, rolling mean\n",
        "- `epsilon_greedy`: pilih aksi berdasarkan Q-table diskrit.\n",
        "- `evaluate`: rata-rata reward per episode untuk policy greedy.\n",
        "- `rolling_mean`: smoothing plot reward."
      ],
      "id": "NSw9Cc16uYTn"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y3yfdqycuYTn"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(Q, state_idx, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randrange(n_actions)\n",
        "    return int(np.argmax(Q[state_idx]))\n",
        "\n",
        "def evaluate(Q, episodes=20):\n",
        "    \"\"\"Evaluasi policy greedy dari Q, kembalikan rata-rata total reward per episode.\"\"\"\n",
        "    tot = 0.0\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        s_idx = discretize_state(s)\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        while not done:\n",
        "            a = int(np.argmax(Q[s_idx]))\n",
        "            s, r, terminated, truncated, _ = env.step(a)\n",
        "            s_idx = discretize_state(s)\n",
        "            ep_reward += r\n",
        "            done = terminated or truncated\n",
        "        tot += ep_reward\n",
        "    return tot / episodes\n",
        "\n",
        "def rolling_mean(x, w=50):\n",
        "    if len(x) < w:\n",
        "        return np.array(x, dtype=float)\n",
        "    c = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (c[w:] - c[:-w]) / float(w)"
      ],
      "id": "y3yfdqycuYTn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tksrH4_OuYTo"
      },
      "source": [
        "## 4) SARSA (on-policy)\n",
        "Update memakai aksi berikutnya yang benar-benar diambil (on-policy)."
      ],
      "id": "tksrH4_OuYTo"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QH0y5yreuYTp"
      },
      "outputs": [],
      "source": [
        "def train_sarsa(\n",
        "    episodes=2000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    eps_start=1.0,\n",
        "    eps_end=0.05,\n",
        "):\n",
        "    Q = np.zeros(q_shape(), dtype=float)\n",
        "    rewards = []\n",
        "    eval_hist = []\n",
        "\n",
        "    for ep in range(1, episodes+1):\n",
        "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / episodes)\n",
        "        s, _ = env.reset()\n",
        "        s_idx = discretize_state(s)\n",
        "        a = epsilon_greedy(Q, s_idx, epsilon)\n",
        "\n",
        "        done = False\n",
        "        ep_reward = 0.0\n",
        "        while not done:\n",
        "            s_next, r, term, trunc, _ = env.step(a)\n",
        "            ep_reward += r\n",
        "            done = term or trunc\n",
        "            s_next_idx = discretize_state(s_next)\n",
        "            if not done:\n",
        "                a_next = epsilon_greedy(Q, s_next_idx, epsilon)\n",
        "                target = r + gamma * Q[s_next_idx + (a_next,)]\n",
        "            else:\n",
        "                target = r\n",
        "            td_error = target - Q[s_idx + (a,)]\n",
        "            Q[s_idx + (a,)] += alpha * td_error\n",
        "            s_idx, a = s_next_idx, (a_next if not done else 0)\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "        if ep % 50 == 0:\n",
        "            eval_hist.append(evaluate(Q, episodes=10))\n",
        "\n",
        "    return Q, rewards, eval_hist\n"
      ],
      "id": "QH0y5yreuYTp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNGjoC6JuYTp"
      },
      "source": [
        "## 5) Q-learning (off-policy)\n",
        "Update memakai aksi optimal di state berikutnya (off-policy)."
      ],
      "id": "nNGjoC6JuYTp"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uEW-5mvhuYTq"
      },
      "outputs": [],
      "source": [
        "def train_q_learning(\n",
        "    episodes=2000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    eps_start=1.0,\n",
        "    eps_end=0.05,\n",
        "):\n",
        "    Q = np.zeros(q_shape(), dtype=float)\n",
        "    rewards = []\n",
        "    eval_hist = []\n",
        "\n",
        "    for ep in range(1, episodes+1):\n",
        "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / episodes)\n",
        "        s, _ = env.reset()\n",
        "        s_idx = discretize_state(s)\n",
        "        done = False\n",
        "        ep_reward = 0.0\n",
        "        while not done:\n",
        "            a = epsilon_greedy(Q, s_idx, epsilon)\n",
        "            s_next, r, term, trunc, _ = env.step(a)\n",
        "            ep_reward += r\n",
        "            done = term or trunc\n",
        "            s_next_idx = discretize_state(s_next)\n",
        "            best_next = 0.0 if done else np.max(Q[s_next_idx])\n",
        "            target = r + gamma * best_next\n",
        "            td_error = target - Q[s_idx + (a,)]\n",
        "            Q[s_idx + (a,)] += alpha * td_error\n",
        "            s_idx = s_next_idx\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "        if ep % 50 == 0:\n",
        "            eval_hist.append(evaluate(Q, episodes=10))\n",
        "\n",
        "    return Q, rewards, eval_hist\n"
      ],
      "id": "uEW-5mvhuYTq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocWwCH8duYTr"
      },
      "source": [
        "## 6) Latih & Bandingkan\n",
        "Anda bisa memodifikasi hyperparameter untuk melihat pengaruhnya ke performa.\n",
        "Reward per episode menggambarkan lamanya tiang bertahan (maksimal ~500)."
      ],
      "id": "ocWwCH8duYTr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvUC1HFYuYTr"
      },
      "outputs": [],
      "source": [
        "EPISODES = 4000\n",
        "ALPHA = 0.1\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "\n",
        "Q_sarsa, rew_sarsa, eval_sarsa = train_sarsa(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n",
        "Q_q, rew_q, eval_q = train_q_learning(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n",
        "\n",
        "print('Greedy eval (avg reward) SARSA:', evaluate(Q_sarsa, episodes=30))\n",
        "print('Greedy eval (avg reward) Q-learn:', evaluate(Q_q, episodes=30))"
      ],
      "id": "dvUC1HFYuYTr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy1ewR4PuYTs"
      },
      "source": [
        "## 7) Plot Reward per Episode (dengan Rolling Mean)\n",
        "Plot satu per satu agar jelas tren belajarnya."
      ],
      "id": "Hy1ewR4PuYTs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLDbm6riuYTs"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(rew_sarsa)\n",
        "plt.plot(rolling_mean(rew_sarsa, 50))\n",
        "plt.title('SARSA: Reward per Episode (CartPole)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rew_q)\n",
        "plt.plot(rolling_mean(rew_q, 50))\n",
        "plt.title('Q-learning: Reward per Episode (CartPole)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "id": "LLDbm6riuYTs"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "mimetype": "text/x-python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}