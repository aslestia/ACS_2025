{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# TD Learning on CartPole (SARSA & Q-learning)\n", "\n", "Notebook ini menerapkan **Temporal Difference (TD) Learning** pada lingkungan **CartPole-v1** (Gymnasium),\n", "dengan pendekatan **discretization** (mendisretkan state kontinu menjadi indeks diskrit) agar bisa memakai tabel-Q.\n", "\n", "Algoritma:\n", "- **SARSA (on-policy)**\n", "- **Q-learning (off-policy)**\n", "\n", "Kita sertakan komentar rinci agar mudah mengikuti alur kode."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0) Setup & Imports\n", "Jika perlu, jalankan `pip install` untuk memasang Gymnasium.\n", "Anda bisa melewati sel `pip` jika sudah terpasang."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Jika perlu, hapus komentar berikut:\n", "# !pip install gymnasium==0.29.1 numpy matplotlib\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import random\n", "import gymnasium as gym"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Environment: CartPole-v1\n", "CartPole memiliki state kontinyu 4 dimensi: \n", "1) posisi cart, 2) kecepatan cart, 3) sudut tiang, 4) kecepatan sudut tiang.\n", "\n", "Aksi (diskrit): 0 = dorong ke kiri, 1 = dorong ke kanan.\n", "\n", "Episode berakhir jika tiang jatuh melewati batas sudut atau cart melewati batas posisi. Reward = 1 per langkah. Maksimal langkah biasanya 500."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make(\"CartPole-v1\")\n", "n_actions = env.action_space.n\n", "n_actions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Discretization (State Binning)\n", "Karena state kontinu, kita perlu memetakannya ke indeks diskrit agar Q-table bisa dipakai.\n", "\n", "Strategi sederhana:\n", "- Tetapkan rentang (min, max) untuk tiap dimensi (clipping agar stabil).\n", "- Bagi tiap dimensi menjadi sejumlah bin (contoh: [6, 6, 12, 12]).\n", "- Gunakan `np.digitize` untuk memetakan nilai ke indeks bin.\n", "- Gabungkan ke satu indeks tunggal dengan radix/basis (atau simpan sebagai tuple indeks)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Konfigurasi bin untuk tiap dimensi state\n", "NUM_BINS = (6, 6, 12, 12)  # cart_pos, cart_vel, pole_angle, pole_vel\n", "\n", "# Batasan (clipping) untuk tiap dimensi agar diskretisasi tidak meledak\n", "STATE_BOUNDS = np.array([\n", "    [-2.4, 2.4],        # cart position (env termination bound)\n", "    [-3.0, 3.0],        # cart velocity (dibatasi agar masuk akal)\n", "    [-0.2095, 0.2095],  # pole angle (~12 degrees)\n", "    [-3.5, 3.5]         # pole velocity at tip (dibatasi)\n", "], dtype=float)\n", "\n", "def create_bins(low, high, bins):\n", "    \"\"\"Buat batas bin (tanpa termasuk -inf/inf) untuk np.digitize.\"\"\"\n", "    return np.linspace(low, high, bins - 1)\n", "\n", "# Precompute batas bin untuk tiap dimensi\n", "BIN_EDGES = [create_bins(STATE_BOUNDS[i,0], STATE_BOUNDS[i,1], NUM_BINS[i]) for i in range(4)]\n", "\n", "def discretize_state(state):\n", "    \"\"\"Map state kontinu -> tuple indeks diskrit (i0,i1,i2,i3).\"\"\"\n", "    s = np.array(state, dtype=float)\n", "    # Clip agar dalam batas\n", "    s = np.clip(s, STATE_BOUNDS[:,0], STATE_BOUNDS[:,1])\n", "    idxs = [int(np.digitize(s[i], BIN_EDGES[i])) for i in range(4)]\n", "    # Pastikan indeks dalam [0, bins-1]\n", "    idxs = [min(NUM_BINS[i]-1, max(0, idxs[i])) for i in range(4)]\n", "    return tuple(idxs)\n", "\n", "def q_shape():\n", "    return (*NUM_BINS, n_actions)  # contoh: (6,6,12,12,2)\n", "\n", "q_shape()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Utilitas: epsilon-greedy, evaluasi, rolling mean\n", "- `epsilon_greedy`: pilih aksi berdasarkan Q-table diskrit.\n", "- `evaluate`: rata-rata reward per episode untuk policy greedy.\n", "- `rolling_mean`: smoothing plot reward."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def epsilon_greedy(Q, state_idx, epsilon):\n", "    if random.random() < epsilon:\n", "        return random.randrange(n_actions)\n", "    return int(np.argmax(Q[state_idx]))\n", "\n", "def evaluate(Q, episodes=20):\n", "    \"\"\"Evaluasi policy greedy dari Q, kembalikan rata-rata total reward per episode.\"\"\"\n", "    tot = 0.0\n", "    for _ in range(episodes):\n", "        s, _ = env.reset()\n", "        s_idx = discretize_state(s)\n", "        done = False\n", "        ep_reward = 0\n.", "        while not done:\n", "            a = int(np.argmax(Q[s_idx]))\n", "            s, r, terminated, truncated, _ = env.step(a)\n", "            s_idx = discretize_state(s)\n", "            ep_reward += r\n", "            done = terminated or truncated\n", "        tot += ep_reward\n", "    return tot / episodes\n", "\n", "def rolling_mean(x, w=50):\n", "    if len(x) < w:\n", "        return np.array(x, dtype=float)\n", "    c = np.cumsum(np.insert(x, 0, 0))\n", "    return (c[w:] - c[:-w]) / float(w)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) SARSA (on-policy)\n", "Update memakai aksi berikutnya yang benar-benar diambil (on-policy)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_sarsa(\n", "    episodes=2000,\n", "    alpha=0.1,\n", "    gamma=0.99,\n", "    eps_start=1.0,\n", "    eps_end=0.05,\n", "):\n", "    Q = np.zeros(q_shape(), dtype=float)\n", "    rewards = []\n", "    eval_hist = []\n", "\n", "    for ep in range(1, episodes+1):\n", "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / episodes)\n", "        s, _ = env.reset()\n", "        s_idx = discretize_state(s)\n", "        a = epsilon_greedy(Q, s_idx, epsilon)\n", "\n", "        done = False\n", "        ep_reward = 0.0\n", "        while not done:\n", "            s_next, r, term, trunc, _ = env.step(a)\n", "            ep_reward += r\n", "            done = term or trunc\n", "            s_next_idx = discretize_state(s_next)\n", "            if not done:\n", "                a_next = epsilon_greedy(Q, s_next_idx, epsilon)\n", "                target = r + gamma * Q[s_next_idx + (a_next,)]\n", "            else:\n", "                target = r\n", "            td_error = target - Q[s_idx + (a,)]\n", "            Q[s_idx + (a,)] += alpha * td_error\n", "            s_idx, a = s_next_idx, (a_next if not done else 0)\n", "\n", "        rewards.append(ep_reward)\n", "        if ep % 50 == 0:\n", "            eval_hist.append(evaluate(Q, episodes=10))\n", "\n", "    return Q, rewards, eval_hist\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Q-learning (off-policy)\n", "Update memakai aksi optimal di state berikutnya (off-policy)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_q_learning(\n", "    episodes=2000,\n", "    alpha=0.1,\n", "    gamma=0.99,\n", "    eps_start=1.0,\n", "    eps_end=0.05,\n", "):\n", "    Q = np.zeros(q_shape(), dtype=float)\n", "    rewards = []\n", "    eval_hist = []\n", "\n", "    for ep in range(1, episodes+1):\n", "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / episodes)\n", "        s, _ = env.reset()\n", "        s_idx = discretize_state(s)\n", "        done = False\n", "        ep_reward = 0.0\n", "        while not done:\n", "            a = epsilon_greedy(Q, s_idx, epsilon)\n", "            s_next, r, term, trunc, _ = env.step(a)\n", "            ep_reward += r\n", "            done = term or trunc\n", "            s_next_idx = discretize_state(s_next)\n", "            best_next = 0.0 if done else np.max(Q[s_next_idx])\n", "            target = r + gamma * best_next\n", "            td_error = target - Q[s_idx + (a,)]\n", "            Q[s_idx + (a,)] += alpha * td_error\n", "            s_idx = s_next_idx\n", "\n", "        rewards.append(ep_reward)\n", "        if ep % 50 == 0:\n", "            eval_hist.append(evaluate(Q, episodes=10))\n", "\n", "    return Q, rewards, eval_hist\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Latih & Bandingkan\n", "Anda bisa memodifikasi hyperparameter untuk melihat pengaruhnya ke performa.\n", "Reward per episode menggambarkan lamanya tiang bertahan (maksimal ~500)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["EPISODES = 4000\n", "ALPHA = 0.1\n", "GAMMA = 0.99\n", "EPS_START = 1.0\n", "EPS_END = 0.05\n", "\n", "Q_sarsa, rew_sarsa, eval_sarsa = train_sarsa(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n", "Q_q, rew_q, eval_q = train_q_learning(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n", "\n", "print('Greedy eval (avg reward) SARSA:', evaluate(Q_sarsa, episodes=30))\n", "print('Greedy eval (avg reward) Q-learn:', evaluate(Q_q, episodes=30))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Plot Reward per Episode (dengan Rolling Mean)\n", "Plot satu per satu agar jelas tren belajarnya."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "plt.plot(rew_sarsa)\n", "plt.plot(rolling_mean(rew_sarsa, 50))\n", "plt.title('SARSA: Reward per Episode (CartPole)')\n", "plt.xlabel('Episode')\n", "plt.ylabel('Total Reward')\n", "plt.show()\n", "\n", "plt.figure()\n", "plt.plot(rew_q)\n", "plt.plot(rolling_mean(rew_q, 50))\n", "plt.title('Q-learning: Reward per Episode (CartPole)')\n", "plt.xlabel('Episode')\n", "plt.ylabel('Total Reward')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Catatan & Variasi\n", "- Discretization mempermudah implementasi tabel-Q, tetapi ada trade-off **quantization error**.\n", "- Anda bisa meningkatkan jumlah bin untuk dimensi tertentu (misal sudut tiang) agar lebih presisi, namun Q-table membesar.\n", "- Alternatif yang lebih kuat: **function approximation** (linear, tile-coding) atau **Deep Q-Network (DQN)** jika ingin menghindari diskretisasi."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "mimetype": "text/x-python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}