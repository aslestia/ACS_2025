{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslestia/ACS_2025/blob/main/Week7_CVaR_PG_CliffWalking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4404a487",
      "metadata": {
        "id": "4404a487"
      },
      "source": [
        "\n",
        "# Week 7 Practice 1 — CVaR Policy Gradient on CliffWalking\n",
        "\n",
        "Tujuan:\n",
        "- Membandingkan **REINFORCE (risk‑neutral)** vs **CVaR‑Policy‑Gradient (risk‑averse)**.\n",
        "- Metrik evaluasi: mean return, VaR$_\\alpha$, CVaR$_\\alpha$, dan frekuensi jatuh (cliff).\n",
        "\n",
        "> Catatan: Notebook ini menggunakan *environment* CliffWalking sederhana yang diimplementasikan manual agar dapat berjalan tanpa instalasi eksternal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eb45ea8",
      "metadata": {
        "id": "6eb45ea8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math, random, statistics, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List, Dict\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# ====== Utility: VaR and CVaR (empirical) ======\n",
        "def var_cvar(samples: np.ndarray, alpha: float=0.95):\n",
        "    samples = np.asarray(samples).copy()\n",
        "    samples.sort()\n",
        "    # Assuming larger value = better return (rewards). For tail risk of losses,\n",
        "    # flip sign accordingly. Here we work with returns, so we compute lower-tail VaR.\n",
        "    q_idx = int(math.floor(alpha * (len(samples)-1)))\n",
        "    var = samples[q_idx]\n",
        "    tail = samples[:q_idx+1]  # lower tail up to VaR (risk of poor returns)\n",
        "    cvar = tail.mean() if len(tail) else var\n",
        "    return var, cvar\n",
        "\n",
        "# ====== Simple CliffWalking Environment (gridworld) ======\n",
        "# Grid (rows x cols); start at (rows-1,0), goal at (rows-1, cols-1)\n",
        "# Cells between start and goal on bottom row are CLIFF (terminal with large negative reward)\n",
        "class CliffWalking:\n",
        "    def __init__(self, rows=4, cols=12, step_reward=-1.0, cliff_reward=-100.0, goal_reward=0.0, max_steps=200):\n",
        "        self.rows, self.cols = rows, cols\n",
        "        self.start = (rows-1, 0)\n",
        "        self.goal = (rows-1, cols-1)\n",
        "        self.step_reward = step_reward\n",
        "        self.cliff_reward = cliff_reward\n",
        "        self.goal_reward = goal_reward\n",
        "        self.max_steps = max_steps\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.s = self.start\n",
        "        self.t = 0\n",
        "        return self.state_id(self.s)\n",
        "\n",
        "    def state_id(self, pos: Tuple[int,int]):\n",
        "        r,c = pos\n",
        "        return r*self.cols + c\n",
        "\n",
        "    @property\n",
        "    def nS(self): return self.rows*self.cols\n",
        "    @property\n",
        "    def nA(self): return 4  # up, right, down, left\n",
        "\n",
        "    def step(self, a: int):\n",
        "        r,c = self.s\n",
        "        if a==0: r = max(0, r-1)\n",
        "        elif a==1: c = min(self.cols-1, c+1)\n",
        "        elif a==2: r = min(self.rows-1, r+1)\n",
        "        elif a==3: c = max(0, c-1)\n",
        "\n",
        "        self.t += 1\n",
        "        # Cliff check\n",
        "        if r==self.rows-1 and 0 < c < self.cols-1:\n",
        "            # Fell into cliff: terminal\n",
        "            self.s = self.start\n",
        "            return self.state_id(self.s), self.cliff_reward, True, {\"fell\": True}\n",
        "        # Goal check\n",
        "        if (r,c)==self.goal:\n",
        "            self.s = (r,c)\n",
        "            return self.state_id(self.s), self.goal_reward, True, {\"fell\": False}\n",
        "        # Normal step\n",
        "        self.s = (r,c)\n",
        "        done = self.t >= self.max_steps\n",
        "        return self.state_id(self.s), self.step_reward, done, {\"fell\": False}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b19da6d",
      "metadata": {
        "id": "3b19da6d"
      },
      "source": [
        "\n",
        "## Policy: Softmax tabular\n",
        "Parameter $\\theta \\in \\mathbb{R}^{|S|\\times|A|}$;  \n",
        "$\\pi_\\theta(a|s) = \\mathrm{softmax}(\\theta_{s, :})[a]$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b8b90c",
      "metadata": {
        "id": "06b8b90c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SoftmaxPolicy:\n",
        "    def __init__(self, nS, nA, init_std=0.01):\n",
        "        self.nS, self.nA = nS, nA\n",
        "        self.theta = np.random.randn(nS, nA) * init_std\n",
        "\n",
        "    def probs(self, s):\n",
        "        z = self.theta[s] - self.theta[s].max()\n",
        "        exp = np.exp(z)\n",
        "        return exp/exp.sum()\n",
        "\n",
        "    def sample(self, s):\n",
        "        p = self.probs(s)\n",
        "        return np.random.choice(self.nA, p=p)\n",
        "\n",
        "    def grad_logp(self, s, a):\n",
        "        p = self.probs(s)\n",
        "        g = -np.outer(np.ones(self.nA), p).T  # placeholder to keep shape\n",
        "        # Simpler: grad for softmax logp = onehot(a) - p\n",
        "        grad = -p\n",
        "        grad[a] += 1.0\n",
        "        out = np.zeros_like(self.theta)\n",
        "        out[s] = grad\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae21aed0",
      "metadata": {
        "id": "ae21aed0"
      },
      "source": [
        "\n",
        "## REINFORCE (Risk‑Neutral) and CVaR‑PG (Risk‑Averse)\n",
        "\n",
        "- **REINFORCE** update: $\\theta \\leftarrow \\theta + \\beta\\; \\nabla_\\theta \\log \\pi(a|s)\\; G$  \n",
        "- **CVaR surrogate (Rockafellar–Uryasev)** with parameter $\\eta$ (approx VaR):  \n",
        "  $$L_\\alpha(\\theta,\\eta) = \\eta + \\tfrac{1}{1-\\alpha}\\, \\mathbb{E}\\big[(G-\\eta)^+\\big]$$\n",
        "  $$\\nabla_\\theta L = \\tfrac{1}{1-\\alpha} \\mathbb{E}[\\nabla\\log\\pi\\; (G-\\eta)^+]$$\n",
        "  $$\\nabla_\\eta L = 1 - \\tfrac{1}{1-\\alpha} \\Pr(G \\ge \\eta)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06387ee",
      "metadata": {
        "id": "f06387ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_episode(env: CliffWalking, policy: SoftmaxPolicy, gamma=1.0):\n",
        "    s = env.reset()\n",
        "    states, actions, rewards, fell_flags = [], [], [], []\n",
        "    done = False\n",
        "    while not done:\n",
        "        a = policy.sample(s)\n",
        "        s2, r, done, info = env.step(a)\n",
        "        states.append(s); actions.append(a); rewards.append(r); fell_flags.append(info.get(\"fell\", False))\n",
        "        s = s2\n",
        "    # return-to-go G (sum of rewards)\n",
        "    G = sum(rewards[i]* (gamma**i) for i in range(len(rewards)))\n",
        "    return states, actions, rewards, G, any(fell_flags)\n",
        "\n",
        "def train_reinforce(episodes=2000, alpha=0.01, gamma=1.0):\n",
        "    env = CliffWalking()\n",
        "    pi = SoftmaxPolicy(env.nS, env.nA)\n",
        "    returns, fell_hist = [], []\n",
        "    for ep in range(episodes):\n",
        "        states, actions, rewards, G, fell = run_episode(env, pi, gamma)\n",
        "        returns.append(G); fell_hist.append(fell)\n",
        "        # REINFORCE update\n",
        "        for s,a in zip(states, actions):\n",
        "            grad = pi.grad_logp(s,a)\n",
        "            pi.theta += alpha * G * grad\n",
        "        if (ep+1)%200==0:\n",
        "            print(f\"[REINFORCE] ep {ep+1}, mean G(last200)={np.mean(returns[-200:]):.2f}\")\n",
        "    return pi, np.array(returns), np.array(fell_hist)\n",
        "\n",
        "def train_cvar_pg(episodes=2000, alpha_theta=0.01, alpha_eta=0.05, gamma=1.0, alpha=0.95):\n",
        "    env = CliffWalking()\n",
        "    pi = SoftmaxPolicy(env.nS, env.nA)\n",
        "    eta = -10.0  # initial VaR proxy\n",
        "    returns, fell_hist, etas = [], [], []\n",
        "    for ep in range(episodes):\n",
        "        states, actions, rewards, G, fell = run_episode(env, pi, gamma)\n",
        "        returns.append(G); fell_hist.append(fell); etas.append(eta)\n",
        "        # CVaR-PG update\n",
        "        tail_term = max(G - eta, 0.0)\n",
        "        for s,a in zip(states, actions):\n",
        "            grad = pi.grad_logp(s,a)\n",
        "            pi.theta += (alpha_theta/(1.0 - alpha)) * tail_term * grad\n",
        "        # eta update (stochastic ascent on -L -> descent on L)\n",
        "        # gradient of L wrt eta: 1 - (1/(1-alpha)) * 1{G >= eta}\n",
        "        indicator = 1.0 if G >= eta else 0.0\n",
        "        grad_eta = 1.0 - indicator/(1.0 - alpha)\n",
        "        eta -= alpha_eta * grad_eta  # gradient descent on L\n",
        "        if (ep+1)%200==0:\n",
        "            print(f\"[CVaR-PG] ep {ep+1}, eta={eta:.2f}, mean G(last200)={np.mean(returns[-200:]):.2f}\")\n",
        "    return pi, np.array(returns), np.array(fell_hist), np.array(etas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530ab794",
      "metadata": {
        "id": "530ab794"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Train both agents ======\n",
        "reinforce_pi, r_returns, r_fell = train_reinforce(episodes=1500, alpha=0.02)\n",
        "cvar_pi, c_returns, c_fell, c_etas = train_cvar_pg(episodes=1500, alpha_theta=0.02, alpha_eta=0.1, alpha=0.95)\n",
        "\n",
        "def summarize(name, returns, fell, alpha=0.95):\n",
        "    mean = returns.mean()\n",
        "    var, cvar = var_cvar(returns, alpha=1-alpha)  # lower-tail (poor returns): use 1-alpha quantile\n",
        "    print(f\"{name}: mean={mean:.2f}, lower-tail VaR_{1-alpha:.2f}={var:.2f}, CVaR_{1-alpha:.2f}={cvar:.2f}, fell_rate={fell.mean():.3f}\")\n",
        "\n",
        "print()\n",
        "summarize(\"REINFORCE\", r_returns, r_fell, alpha=0.95)\n",
        "summarize(\"CVaR-PG\", c_returns, c_fell, alpha=0.95)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cba60f8",
      "metadata": {
        "id": "2cba60f8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Visualization: Returns histogram ======\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.hist(r_returns, bins=40, alpha=0.6, label=\"REINFORCE\")\n",
        "plt.hist(c_returns, bins=40, alpha=0.6, label=\"CVaR-PG\")\n",
        "plt.axvline(np.mean(r_returns), linestyle=\"--\", label=\"Mean RN\")\n",
        "plt.axvline(np.mean(c_returns), linestyle=\"-.\", label=\"Mean CVaR\")\n",
        "plt.title(\"Distribution of returns\")\n",
        "plt.xlabel(\"Episode return\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b659ba4c",
      "metadata": {
        "id": "b659ba4c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Visualization: Eta trajectory (CVaR-PG) ======\n",
        "plt.figure(figsize=(7,3.5))\n",
        "plt.plot(c_etas)\n",
        "plt.title(\"Eta (VaR proxy) during CVaR-PG training\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"eta\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}