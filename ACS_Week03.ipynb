{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslestia/ACS_2025/blob/main/ACS_Week03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jika perlu:\n",
        "# !pip install gymnasium==0.29.1 matplotlib numpy\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "#==Environment & Fixed Policy (untuk evaluasi)==\n",
        "env = gym.make(\"Blackjack-v1\", sab=True)  # episodic, no counting cards\n",
        "\n",
        "def fixed_policy(observation):\n",
        "    # observation = (player_sum, dealer_showing, usable_ace)\n",
        "    player_sum, dealer, usable_ace = observation\n",
        "    return 0 if player_sum >= 20 else 1  # 0: stick, 1: hit\n",
        "#== Generate Episode (untuk MC)\n",
        "def generate_episode(policy):\n",
        "    episode = []\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        a = policy(obs)\n",
        "        next_obs, r, terminated, truncated, info = env.step(a)\n",
        "        episode.append((obs, a, r))\n",
        "        obs = next_obs\n",
        "        done = terminated or truncated\n",
        "    return episode\n",
        "#== First-Visit MC Policy Evaluation (menghitung V dan Q)\n",
        "def mc_prediction_first_visit(policy, num_episodes=100000, gamma=1.0):\n",
        "    # Q: dict[(state, action)] -> value\n",
        "    returns_sum = {}   # jumlah G untuk (s,a)\n",
        "    returns_count = {} # banyaknya kunjungan pertama (s,a)\n",
        "    Q = {}\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        episode = generate_episode(policy)\n",
        "\n",
        "        # hitung return G_t mundur\n",
        "        G = 0.0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = gamma * G + r\n",
        "\n",
        "            # first-visit check\n",
        "            if (s, a) not in visited:\n",
        "                visited.add((s, a))\n",
        "                returns_sum[(s, a)] = returns_sum.get((s, a), 0.0) + G\n",
        "                returns_count[(s, a)] = returns_count.get((s, a), 0) + 1\n",
        "                Q[(s, a)] = returns_sum[(s, a)] / returns_count[(s, a)]\n",
        "\n",
        "    return Q\n",
        "#== Visualisasi Nilai Policy\n",
        "def value_from_Q(Q, usable_ace=False):\n",
        "    V = np.full((32, 12), np.nan)  # player_sum 0..31, dealer 0..11\n",
        "    for ps in range(12, 22):       # biasanya relevan 12..21\n",
        "        for dealer in range(1, 11):\n",
        "            s = (ps, dealer, usable_ace)\n",
        "            Qa = [Q.get((s, 0), None), Q.get((s, 1), None)]\n",
        "            if any(v is not None for v in Qa):\n",
        "                V[ps, dealer] = np.nanmax([v if v is not None else -np.inf for v in Qa])\n",
        "    return V\n",
        "\n",
        "Q_eval = mc_prediction_first_visit(fixed_policy, num_episodes=50000, gamma=1.0)\n",
        "V_noace = value_from_Q(Q_eval, usable_ace=False)\n",
        "V_ace   = value_from_Q(Q_eval, usable_ace=True)\n",
        "\n",
        "plt.figure(); plt.imshow(V_noace.T, origin='lower', aspect='auto')\n",
        "plt.title(\"V(s) MC Evaluation – usable_ace=False\"); plt.xlabel(\"player_sum\"); plt.ylabel(\"dealer_showing\"); plt.colorbar(); plt.show()\n",
        "\n",
        "plt.figure(); plt.imshow(V_ace.T, origin='lower', aspect='auto')\n",
        "plt.title(\"V(s) MC Evaluation – usable_ace=True\"); plt.xlabel(\"player_sum\"); plt.ylabel(\"dealer_showing\"); plt.colorbar(); plt.show()\n",
        "\n",
        "#== MC Control (ε-greedy) – Mencari Policy yang Lebih Baik\n",
        "def mc_control_epsilon_greedy(num_episodes=500000, gamma=1.0, epsilon_start=1.0, epsilon_end=0.05):\n",
        "    Q = {}\n",
        "    returns_sum, returns_count = {}, {}\n",
        "\n",
        "    def epsilon_greedy_action(s, eps):\n",
        "        # actions: 0 stick, 1 hit\n",
        "        q0 = Q.get((s,0), 0.0)\n",
        "        q1 = Q.get((s,1), 0.0)\n",
        "        if random.random() < eps:\n",
        "            return random.choice([0,1])\n",
        "        return 0 if q0 >= q1 else 1\n",
        "\n",
        "    for i in range(1, num_episodes+1):\n",
        "        # linear decay epsilon\n",
        "        eps = max(epsilon_end, epsilon_start - (epsilon_start - epsilon_end) * (i/num_episodes))\n",
        "\n",
        "        # generate episode with current ε-greedy policy\n",
        "        episode = []\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = epsilon_greedy_action(s, eps)\n",
        "            s_next, r, terminated, truncated, _ = env.step(a)\n",
        "            episode.append((s, a, r))\n",
        "            s = s_next\n",
        "            done = terminated or truncated\n",
        "\n",
        "        # First-visit MC updates\n",
        "        G = 0.0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = gamma * G + r\n",
        "            if (s,a) not in visited:\n",
        "                visited.add((s,a))\n",
        "                returns_sum[(s,a)] = returns_sum.get((s,a), 0.0) + G\n",
        "                returns_count[(s,a)] = returns_count.get((s,a), 0) + 1\n",
        "                Q[(s,a)] = returns_sum[(s,a)] / returns_count[(s,a)]\n",
        "\n",
        "    # extract greedy policy\n",
        "    def greedy_policy(s):\n",
        "        q0 = Q.get((s,0), 0.0); q1 = Q.get((s,1), 0.0)\n",
        "        return 0 if q0 >= q1 else 1\n",
        "\n",
        "    return Q, greedy_policy\n",
        "#== Evaluasi Singkat Policy Hasil MC Control\n",
        "Q_mc, pi_mc = mc_control_epsilon_greedy(num_episodes=200000)\n",
        "\n",
        "def evaluate_policy(policy, n_episodes=10000):\n",
        "    rewards = []\n",
        "    for _ in range(n_episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        G = 0\n",
        "        while not done:\n",
        "            a = policy(s)\n",
        "            s, r, term, trunc, _ = env.step(a)\n",
        "            G += r\n",
        "            done = term or trunc\n",
        "        rewards.append(G)\n",
        "    return np.mean(rewards)\n",
        "\n",
        "avg_return_fixed = evaluate_policy(fixed_policy, 5000)\n",
        "avg_return_mc    = evaluate_policy(pi_mc, 5000)\n",
        "print(\"Average return Fixed:\", avg_return_fixed)\n",
        "print(\"Average return MC-control greedy:\", avg_return_mc)\n"
      ],
      "metadata": {
        "id": "Ioen46lxYLIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gymnasium==0.29.1 numpy matplotlib\n",
        "\n",
        "import numpy as np, random\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"Blackjack-v1\", sab=True)  # episodic\n",
        "ACTIONS = [0, 1]  # 0=stick, 1=hit\n",
        "def evaluate_policy(policy_fn, n_episodes=5000, env=env):\n",
        "    \"\"\"Return rata-rata reward episode dengan policy_fn(s)->a (greedy).\"\"\"\n",
        "    returns = []\n",
        "    for _ in range(n_episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        G = 0\n",
        "        while not done:\n",
        "            a = policy_fn(s)\n",
        "            s, r, term, trunc, _ = env.step(a)\n",
        "            G += r\n",
        "            done = term or trunc\n",
        "        returns.append(G)\n",
        "    return float(np.mean(returns))\n",
        "\n",
        "def greedy_from_Q(Q):\n",
        "    def pi(s):\n",
        "        q0 = Q.get((s,0), 0.0)\n",
        "        q1 = Q.get((s,1), 0.0)\n",
        "        return 0 if q0 >= q1 else 1\n",
        "    return pi\n",
        "\n",
        "def eps_greedy_action(Q, s, eps):\n",
        "    if random.random() < eps:\n",
        "        return random.choice(ACTIONS)\n",
        "    q0 = Q.get((s,0), 0.0); q1 = Q.get((s,1), 0.0)\n",
        "    return 0 if q0 >= q1 else 1\n",
        "def mc_control_eps_greedy_first_visit(\n",
        "    num_episodes=200_000, gamma=1.0, eps_start=1.0, eps_end=0.05, env=env, eval_every=10_000\n",
        "):\n",
        "    Q = {}\n",
        "    ret_sum, ret_cnt = {}, {}\n",
        "    eval_points, eval_scores = [], []\n",
        "\n",
        "    for ep in range(1, num_episodes+1):\n",
        "        eps = max(eps_end, eps_start - (eps_start-eps_end)*ep/num_episodes)\n",
        "        # generate satu episode dengan policy ε-greedy saat ini\n",
        "        episode = []\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = eps_greedy_action(Q, s, eps)\n",
        "            s2, r, term, trunc, _ = env.step(a)\n",
        "            episode.append((s, a, r))\n",
        "            s = s2\n",
        "            done = term or trunc\n",
        "\n",
        "        # First-visit updates (gunakan set visited)\n",
        "        G = 0.0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = gamma*G + r\n",
        "            if (s,a) not in visited:\n",
        "                visited.add((s,a))\n",
        "                ret_sum[(s,a)] = ret_sum.get((s,a), 0.0) + G\n",
        "                ret_cnt[(s,a)] = ret_cnt.get((s,a), 0) + 1\n",
        "                Q[(s,a)] = ret_sum[(s,a)] / ret_cnt[(s,a)]\n",
        "\n",
        "        if ep % eval_every == 0:\n",
        "            pi_greedy = greedy_from_Q(Q)\n",
        "            score = evaluate_policy(pi_greedy, n_episodes=3000, env=env)\n",
        "            eval_points.append(ep); eval_scores.append(score)\n",
        "\n",
        "    return Q, (eval_points, eval_scores)\n",
        "def mc_control_eps_greedy_every_visit(\n",
        "    num_episodes=200_000, gamma=1.0, eps_start=1.0, eps_end=0.05, env=env, eval_every=10_000\n",
        "):\n",
        "    Q = {}\n",
        "    ret_sum, ret_cnt = {}, {}\n",
        "    eval_points, eval_scores = [], []\n",
        "\n",
        "    for ep in range(1, num_episodes+1):\n",
        "        eps = max(eps_end, eps_start - (eps_start-eps_end)*ep/num_episodes)\n",
        "        episode = []\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = eps_greedy_action(Q, s, eps)\n",
        "            s2, r, term, trunc, _ = env.step(a)\n",
        "            episode.append((s, a, r))\n",
        "            s = s2\n",
        "            done = term or trunc\n",
        "\n",
        "        # Every-visit updates: TIDAK pakai set visited\n",
        "        G = 0.0\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = gamma*G + r\n",
        "            ret_sum[(s,a)] = ret_sum.get((s,a), 0.0) + G\n",
        "            ret_cnt[(s,a)] = ret_cnt.get((s,a), 0) + 1\n",
        "            Q[(s,a)] = ret_sum[(s,a)] / ret_cnt[(s,a)]\n",
        "\n",
        "        if ep % eval_every == 0:\n",
        "            pi_greedy = greedy_from_Q(Q)\n",
        "            score = evaluate_policy(pi_greedy, n_episodes=3000, env=env)\n",
        "            eval_points.append(ep); eval_scores.append(score)\n",
        "\n",
        "    return Q, (eval_points, eval_scores)\n",
        "EPISODES = 100_000\n",
        "Q_fv, (x_fv, y_fv) = mc_control_eps_greedy_first_visit(num_episodes=EPISODES, eval_every=10_000)\n",
        "Q_ev, (x_ev, y_ev) = mc_control_eps_greedy_every_visit(num_episodes=EPISODES, eval_every=10_000)\n",
        "\n",
        "pi_fv = greedy_from_Q(Q_fv)\n",
        "pi_ev = greedy_from_Q(Q_ev)\n",
        "\n",
        "avg_fv = evaluate_policy(pi_fv, n_episodes=10_000)\n",
        "avg_ev = evaluate_policy(pi_ev, n_episodes=10_000)\n",
        "\n",
        "print(\"Average return (greedy) – First-visit:\", round(avg_fv, 4))\n",
        "print(\"Average return (greedy) – Every-visit:\", round(avg_ev, 4))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(x_fv, y_fv, marker='o', label='First-visit MC')\n",
        "plt.plot(x_ev, y_ev, marker='s', label='Every-visit MC')\n",
        "plt.axhline(0.0, linestyle='--', linewidth=1)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Avg return (eval 3k eps)\")\n",
        "plt.title(\"MC Control ε-greedy: First-visit vs Every-visit\")\n",
        "plt.legend(); plt.show()\n"
      ],
      "metadata": {
        "id": "EhXwE3slIe05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31V3wA7Ofv-E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jika diperlukan, buka komentar baris berikut untuk memasang dependensi:\n",
        "# !pip install gymnasium==0.29.1 numpy matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "n_states, n_actions\n",
        "\n",
        "def epsilon_greedy(Q, s, epsilon):\n",
        "    \"\"\"Pilih aksi dengan epsilon-greedy dari tabel Q.\n",
        "    - Dengan probabilitas epsilon: pilih aksi acak (eksplorasi)\n",
        "    - Selainnya: pilih aksi argmax Q (eksploitasi)\n",
        "    \"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return random.randrange(n_actions)\n",
        "    else:\n",
        "        return int(np.argmax(Q[s]))\n",
        "\n",
        "def evaluate_policy(Q, episodes=500):\n",
        "    \"\"\"Evaluasi policy greedy(Q): kembalikan rata-rata reward (success rate).\"\"\"\n",
        "    wins = 0\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = int(np.argmax(Q[s]))\n",
        "            s, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            if done and r > 0:\n",
        "                wins += 1\n",
        "    return wins / episodes\n",
        "\n",
        "def rolling_mean(x, w=50):\n",
        "    if len(x) < w:\n",
        "        return np.array(x, dtype=float)\n",
        "    c = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (c[w:] - c[:-w]) / float(w)\n",
        "\n",
        "#=================\n",
        "\n",
        "def train_sarsa(\n",
        "    num_episodes=5000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    eps_start=1.0,\n",
        "    eps_end=0.05,\n",
        "):\n",
        "    Q = np.zeros((n_states, n_actions), dtype=float)\n",
        "    eps_history = []\n",
        "    perf_history = []  # success rate evaluasi berkala\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        # Linear decay epsilon\n",
        "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / num_episodes)\n",
        "        eps_history.append(epsilon)\n",
        "\n",
        "        s, _ = env.reset()\n",
        "        a = epsilon_greedy(Q, s, epsilon)\n",
        "        done = False\n",
        "        while not done:\n",
        "            s2, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            if not done:\n",
        "                a2 = epsilon_greedy(Q, s2, epsilon)\n",
        "                target = r + gamma * Q[s2, a2]\n",
        "            else:\n",
        "                target = r  # terminal\n",
        "            td_error = target - Q[s, a]\n",
        "            Q[s, a] += alpha * td_error\n",
        "            s, a = s2, (epsilon_greedy(Q, s2, epsilon) if not done else 0)\n",
        "\n",
        "        # evaluasi berkala (setiap 200 ep)\n",
        "        if ep % 200 == 0:\n",
        "            perf = evaluate_policy(Q, episodes=300)\n",
        "            perf_history.append(perf)\n",
        "\n",
        "    return Q, eps_history, perf_history\n",
        "#=============================\n",
        "def train_q_learning(\n",
        "    num_episodes=5000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    eps_start=1.0,\n",
        "    eps_end=0.05,\n",
        "):\n",
        "    Q = np.zeros((n_states, n_actions), dtype=float)\n",
        "    eps_history = []\n",
        "    perf_history = []\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / num_episodes)\n",
        "        eps_history.append(epsilon)\n",
        "\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = epsilon_greedy(Q, s, epsilon)\n",
        "            s2, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            best_next = 0 if done else np.max(Q[s2])\n",
        "            target = r + gamma * best_next\n",
        "            td_error = target - Q[s, a]\n",
        "            Q[s, a] += alpha * td_error\n",
        "            s = s2\n",
        "\n",
        "        if ep % 200 == 0:\n",
        "            perf = evaluate_policy(Q, episodes=300)\n",
        "            perf_history.append(perf)\n",
        "\n",
        "    return Q, eps_history, perf_history\n",
        "#==============\n",
        "\n",
        "EPISODES = 10000\n",
        "ALPHA = 0.1\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "\n",
        "Q_sarsa, eps_sarsa, perf_sarsa = train_sarsa(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n",
        "Q_q, eps_q, perf_q = train_q_learning(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n",
        "\n",
        "avg_sarsa = evaluate_policy(Q_sarsa, episodes=1000)\n",
        "avg_q = evaluate_policy(Q_q, episodes=1000)\n",
        "avg_sarsa, avg_q\n",
        "\n",
        "#===============\n",
        "\n",
        "x = np.arange(200, EPISODES+1, 200)\n",
        "plt.figure()\n",
        "plt.plot(x, perf_sarsa, marker='o', label='SARSA')\n",
        "plt.plot(x, perf_q, marker='s', label='Q-learning')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Success rate (eval 300 eps)')\n",
        "plt.title('FrozenLake TD Control: SARSA vs Q-learning')\n",
        "plt.legend(); plt.show()"
      ],
      "metadata": {
        "id": "Xwzbx28Ca7nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jika perlu, hapus komentar berikut:\n",
        "# !pip install gymnasium==0.29.1 numpy matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "n_actions = env.action_space.n\n",
        "n_actions\n",
        "\n",
        "#===\n",
        "# Konfigurasi bin untuk tiap dimensi state\n",
        "NUM_BINS = (6, 6, 12, 12)  # cart_pos, cart_vel, pole_angle, pole_vel\n",
        "\n",
        "# Batasan (clipping) untuk tiap dimensi agar diskretisasi tidak meledak\n",
        "STATE_BOUNDS = np.array([\n",
        "    [-2.4, 2.4],        # cart position (env termination bound)\n",
        "    [-3.0, 3.0],        # cart velocity (dibatasi agar masuk akal)\n",
        "    [-0.2095, 0.2095],  # pole angle (~12 degrees)\n",
        "    [-3.5, 3.5]         # pole velocity at tip (dibatasi)\n",
        "], dtype=float)\n",
        "\n",
        "def create_bins(low, high, bins):\n",
        "    \"\"\"Buat batas bin (tanpa termasuk -inf/inf) untuk np.digitize.\"\"\"\n",
        "    return np.linspace(low, high, bins - 1)\n",
        "\n",
        "# Precompute batas bin untuk tiap dimensi\n",
        "BIN_EDGES = [create_bins(STATE_BOUNDS[i,0], STATE_BOUNDS[i,1], NUM_BINS[i]) for i in range(4)]\n",
        "\n",
        "def discretize_state(state):\n",
        "    \"\"\"Map state kontinu -> tuple indeks diskrit (i0,i1,i2,i3).\"\"\"\n",
        "    s = np.array(state, dtype=float)\n",
        "    # Clip agar dalam batas\n",
        "    s = np.clip(s, STATE_BOUNDS[:,0], STATE_BOUNDS[:,1])\n",
        "    idxs = [int(np.digitize(s[i], BIN_EDGES[i])) for i in range(4)]\n",
        "    # Pastikan indeks dalam [0, bins-1]\n",
        "    idxs = [min(NUM_BINS[i]-1, max(0, idxs[i])) for i in range(4)]\n",
        "    return tuple(idxs)\n",
        "\n",
        "def q_shape():\n",
        "    return (*NUM_BINS, n_actions)  # contoh: (6,6,12,12,2)\n",
        "\n",
        "q_shape()\n",
        "\n",
        "#===\n",
        "def epsilon_greedy(Q, state_idx, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randrange(n_actions)\n",
        "    return int(np.argmax(Q[state_idx]))\n",
        "\n",
        "def evaluate(Q, episodes=20):\n",
        "    \"\"\"Evaluasi policy greedy dari Q, kembalikan rata-rata total reward per episode.\"\"\"\n",
        "    tot = 0.0\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        s_idx = discretize_state(s)\n",
        "        done = False\n",
        "        ep_reward = 0.0\n",
        "        while not done:\n",
        "            a = int(np.argmax(Q[s_idx]))\n",
        "            s, r, terminated, truncated, _ = env.step(a)\n",
        "            s_idx = discretize_state(s)\n",
        "            ep_reward += r\n",
        "            done = terminated or truncated\n",
        "        tot += ep_reward\n",
        "    return tot / episodes\n",
        "\n",
        "def rolling_mean(x, w=50):\n",
        "    if len(x) < w:\n",
        "        return np.array(x, dtype=float)\n",
        "    c = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (c[w:] - c[:-w]) / float(w)\n",
        "\n",
        "#===\n",
        "def train_sarsa(\n",
        "    episodes=2000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    eps_start=1.0,\n",
        "    eps_end=0.05,\n",
        "):\n",
        "    Q = np.zeros(q_shape(), dtype=float)\n",
        "    rewards = []\n",
        "    eval_hist = []\n",
        "\n",
        "    for ep in range(1, episodes+1):\n",
        "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / episodes)\n",
        "        s, _ = env.reset()\n",
        "        s_idx = discretize_state(s)\n",
        "        a = epsilon_greedy(Q, s_idx, epsilon)\n",
        "\n",
        "        done = False\n",
        "        ep_reward = 0.0\n",
        "        while not done:\n",
        "            s_next, r, term, trunc, _ = env.step(a)\n",
        "            ep_reward += r\n",
        "            done = term or trunc\n",
        "            s_next_idx = discretize_state(s_next)\n",
        "            if not done:\n",
        "                a_next = epsilon_greedy(Q, s_next_idx, epsilon)\n",
        "                target = r + gamma * Q[s_next_idx + (a_next,)]\n",
        "            else:\n",
        "                target = r\n",
        "            td_error = target - Q[s_idx + (a,)]\n",
        "            Q[s_idx + (a,)] += alpha * td_error\n",
        "            s_idx, a = s_next_idx, (a_next if not done else 0)\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "        if ep % 50 == 0:\n",
        "            eval_hist.append(evaluate(Q, episodes=10))\n",
        "\n",
        "    return Q, rewards, eval_hist\n",
        "\n",
        "#===\n",
        "def train_q_learning(\n",
        "    episodes=2000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    eps_start=1.0,\n",
        "    eps_end=0.05,\n",
        "):\n",
        "    Q = np.zeros(q_shape(), dtype=float)\n",
        "    rewards = []\n",
        "    eval_hist = []\n",
        "\n",
        "    for ep in range(1, episodes+1):\n",
        "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / episodes)\n",
        "        s, _ = env.reset()\n",
        "        s_idx = discretize_state(s)\n",
        "        done = False\n",
        "        ep_reward = 0.0\n",
        "        while not done:\n",
        "            a = epsilon_greedy(Q, s_idx, epsilon)\n",
        "            s_next, r, term, trunc, _ = env.step(a)\n",
        "            ep_reward += r\n",
        "            done = term or trunc\n",
        "            s_next_idx = discretize_state(s_next)\n",
        "            best_next = 0.0 if done else np.max(Q[s_next_idx])\n",
        "            target = r + gamma * best_next\n",
        "            td_error = target - Q[s_idx + (a,)]\n",
        "            Q[s_idx + (a,)] += alpha * td_error\n",
        "            s_idx = s_next_idx\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "        if ep % 50 == 0:\n",
        "            eval_hist.append(evaluate(Q, episodes=10))\n",
        "\n",
        "    return Q, rewards, eval_hist\n",
        "\n",
        "#===\n",
        "EPISODES = 4000\n",
        "ALPHA = 0.1\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "\n",
        "Q_sarsa, rew_sarsa, eval_sarsa = train_sarsa(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n",
        "Q_q, rew_q, eval_q = train_q_learning(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n",
        "\n",
        "print('Greedy eval (avg reward) SARSA:', evaluate(Q_sarsa, episodes=30))\n",
        "print('Greedy eval (avg reward) Q-learn:', evaluate(Q_q, episodes=30))\n",
        "\n",
        "#===\n",
        "plt.figure()\n",
        "plt.plot(rew_sarsa)\n",
        "plt.plot(rolling_mean(rew_sarsa, 50))\n",
        "plt.title('SARSA: Reward per Episode (CartPole)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rew_q)\n",
        "plt.plot(rolling_mean(rew_q, 50))\n",
        "plt.title('Q-learning: Reward per Episode (CartPole)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "55WE6h_Hla1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}