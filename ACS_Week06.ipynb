{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslestia/ACS_2025/blob/main/ACS_Week06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3ca0170",
      "metadata": {
        "id": "c3ca0170"
      },
      "source": [
        "\n",
        "# Week 6 Lab â€” **QR-DQN** (Focus) : Mean vs Quantile Decisions\n",
        "\n",
        "This lab demonstrates **Quantile Regression DQN (QR-DQN)** on a **simple simulated environment**.\n",
        "We compare decisions based on **mean** of predicted quantiles vs **selected quantile** (e.g., median or 75th).\n",
        "\n",
        "**What you'll do:**\n",
        "1. Generate a synthetic environment (choose: `claims` or `stocks`).\n",
        "2. Implement QR-DQN (PyTorch).\n",
        "3. Train the agent to predict the return distribution.\n",
        "4. Compare decisions using **mean** vs **quantile** targets.\n",
        "5. Visualize predicted quantiles per action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b17f656b",
      "metadata": {
        "id": "b17f656b"
      },
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92851b31",
      "metadata": {
        "id": "92851b31"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math, random, collections, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daddb6ce",
      "metadata": {
        "id": "daddb6ce"
      },
      "source": [
        "## 2) Synthetic Environment (claims or stocks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd9a6ab",
      "metadata": {
        "id": "1cd9a6ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    mode: str = 'claims'   # 'claims' or 'stocks'\n",
        "    window: int = 8        # state window length\n",
        "    n_actions: int = 3     # e.g., retention levels or allocation choices\n",
        "    episode_len: int = 64\n",
        "    gamma: float = 0.99\n",
        "\n",
        "cfg = Config()\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df378cd6",
      "metadata": {
        "id": "df378cd6"
      },
      "source": [
        "\n",
        "### Environment dynamics\n",
        "- **claims mode**: sample heavy-tailed losses (Pareto/Lognormal mix). Reward is a simple underwriting-like return:\n",
        "  \\( r_t = \\text{premium} - \\text{retained\\_loss} \\).\n",
        "  Actions set retention levels in \\{0.3, 0.6, 0.9\\}.  \n",
        "- **stocks mode**: sample returns from a mean-reverting + stochastic process; actions represent discrete allocation \\( w\\in\\{0.2,0.5,0.8\\} \\) with reward \\( r_t = w \\cdot R_t \\).\n",
        "State is a rolling window of last observations (loss or return).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f73acf",
      "metadata": {
        "id": "e8f73acf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SimpleSimEnv:\n",
        "    def __init__(self, cfg: Config):\n",
        "        self.cfg = cfg\n",
        "        self.t = 0\n",
        "        self.window = cfg.window\n",
        "        self.series = None\n",
        "        self.reset()\n",
        "\n",
        "    def _gen_series_claims(self, T):\n",
        "        # mixture: with prob 0.5 Lognormal, else Pareto (heavy tail)\n",
        "        ln = np.random.lognormal(mean=2.5, sigma=1.0, size=T)  # moderate body\n",
        "        pa = (np.random.pareto(a=2.0, size=T)+1.0) * 50.0     # heavy tail\n",
        "        u = np.random.rand(T)\n",
        "        losses = np.where(u < 0.5, ln, pa)\n",
        "        return losses  # positive values\n",
        "\n",
        "    def _gen_series_stocks(self, T):\n",
        "        # Simple mean-reverting + noise (not GBM, to keep it minimal)\n",
        "        T0 = T + 100\n",
        "        x = np.zeros(T0)\n",
        "        rho = 0.9\n",
        "        for t in range(1, T0):\n",
        "            x[t] = rho*x[t-1] + 0.05*np.random.randn()\n",
        "        ret = x[100:] + 0.2*np.random.randn(T)  # add noise\n",
        "        return ret  # can be +/-\n",
        "\n",
        "    def reset(self):\n",
        "        T = 4096\n",
        "        if self.cfg.mode == 'claims':\n",
        "            self.series = self._gen_series_claims(T)\n",
        "        else:\n",
        "            self.series = self._gen_series_stocks(T)\n",
        "        # start index ensuring we have a window\n",
        "        self.t = self.window\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        window = self.series[self.t-self.window:self.t]\n",
        "        # normalize for stability\n",
        "        m = np.mean(window) + 1e-8\n",
        "        s = np.std(window) + 1e-8\n",
        "        return ((window - m) / s).astype(np.float32)\n",
        "\n",
        "    def step(self, action:int):\n",
        "        # action to retention/weight\n",
        "        if self.cfg.n_actions == 3:\n",
        "            levels = [0.3, 0.6, 0.9]\n",
        "        else:\n",
        "            levels = np.linspace(0.2, 0.8, self.cfg.n_actions)\n",
        "        level = levels[action]\n",
        "\n",
        "        x = self.series[self.t]\n",
        "        if self.cfg.mode == 'claims':\n",
        "            premium = 50.0\n",
        "            retained_loss = level * x\n",
        "            r = premium - retained_loss\n",
        "        else:  # stocks\n",
        "            r = level * x\n",
        "\n",
        "        self.t += 1\n",
        "        done = (self.t >= len(self.series)) or ((self.t % self.cfg.episode_len) == 0)\n",
        "        next_state = self._get_state()\n",
        "        return next_state, float(r), done, {}\n",
        "\n",
        "env = SimpleSimEnv(cfg)\n",
        "state = env.reset()\n",
        "print(\"state shape:\", state.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2843b486",
      "metadata": {
        "id": "2843b486"
      },
      "source": [
        "## 3) Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe74f35e",
      "metadata": {
        "id": "fe74f35e"
      },
      "outputs": [],
      "source": [
        "\n",
        "Transition = collections.namedtuple('Transition', ('s','a','r','ns','d'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=100_000):\n",
        "        self.buf = collections.deque(maxlen=capacity)\n",
        "    def push(self, *args):\n",
        "        self.buf.append(Transition(*args))\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buf, batch_size)\n",
        "        s = torch.tensor(np.stack([t.s for t in batch]), dtype=torch.float32, device=device)\n",
        "        a = torch.tensor([t.a for t in batch], dtype=torch.long, device=device)\n",
        "        r = torch.tensor([t.r for t in batch], dtype=torch.float32, device=device)\n",
        "        ns = torch.tensor(np.stack([t.ns for t in batch]), dtype=torch.float32, device=device)\n",
        "        d = torch.tensor([t.d for t in batch], dtype=torch.float32, device=device)\n",
        "        return s,a,r,ns,d\n",
        "    def __len__(self):\n",
        "        return len(self.buf)\n",
        "\n",
        "buffer = ReplayBuffer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f47e9cce",
      "metadata": {
        "id": "f47e9cce"
      },
      "source": [
        "## 4) QR-DQN Network & Quantile Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d52872",
      "metadata": {
        "id": "63d52872"
      },
      "outputs": [],
      "source": [
        "\n",
        "class QRQNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions, n_quantiles):\n",
        "        super().__init__()\n",
        "        hidden = 128\n",
        "        self.n_actions = n_actions\n",
        "        self.n_quantiles = n_quantiles\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, n_actions * n_quantiles)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        batch = x.size(0)\n",
        "        out = self.net(x).view(batch, self.n_actions, self.n_quantiles)\n",
        "        return out\n",
        "\n",
        "def huber_quantile_loss(pred, target, taus):\n",
        "    # pred, target: [B, Nq], taus: [Nq]\n",
        "    # compute pairwise td errors between each target and each pred quantile\n",
        "    # We'll follow standard QR-DQN loss (pairwise differences)\n",
        "    B, Nq = pred.shape\n",
        "    with torch.no_grad():\n",
        "        t = target.unsqueeze(1).repeat(1, Nq)  # [B, Nq]\n",
        "    u = t - pred  # TD error\n",
        "    # Huber\n",
        "    kappa = 1.0\n",
        "    abs_u = torch.abs(u)\n",
        "    huber = torch.where(abs_u <= kappa, 0.5*u*u, kappa*(abs_u - 0.5*kappa))\n",
        "    # quantile weight\n",
        "    taus = taus.view(1, Nq).to(pred.device)\n",
        "    loss = torch.abs(taus - (u.detach() < 0).float()) * huber / kappa\n",
        "    return loss.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb2b213c",
      "metadata": {
        "id": "bb2b213c"
      },
      "source": [
        "## 5) Agent, Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99041b27",
      "metadata": {
        "id": "99041b27"
      },
      "outputs": [],
      "source": [
        "\n",
        "class QRDQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, n_quantiles=51, gamma=0.99, lr=1e-3):\n",
        "        self.n_actions = n_actions\n",
        "        self.n_quantiles = n_quantiles\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.taus = torch.linspace(0.0 + 1/(2*n_quantiles), 1.0 - 1/(2*n_quantiles), n_quantiles, device=device)\n",
        "        self.online = QRQNetwork(state_dim, n_actions, n_quantiles).to(device)\n",
        "        self.target = QRQNetwork(state_dim, n_actions, n_quantiles).to(device)\n",
        "        self.target.load_state_dict(self.online.state_dict())\n",
        "        self.opt = optim.Adam(self.online.parameters(), lr=lr)\n",
        "\n",
        "    def act(self, s, eps=0.05, mode='mean'):\n",
        "        if random.random() < eps:\n",
        "            return random.randrange(self.n_actions)\n",
        "        s = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q = self.online(s)[0]  # [A, Nq]\n",
        "            if mode == 'mean':\n",
        "                vals = q.mean(dim=1)  # mean over quantiles\n",
        "            else:\n",
        "                # mode is a tau float, pick nearest quantile\n",
        "                if isinstance(mode, float):\n",
        "                    idx = int(round(mode * (self.n_quantiles-1)))\n",
        "                else:\n",
        "                    idx = self.n_quantiles//2\n",
        "                vals = q[:, idx]\n",
        "            a = int(torch.argmax(vals).item())\n",
        "        return a\n",
        "\n",
        "    def update(self, batch, double=True):\n",
        "        s,a,r,ns,d = batch\n",
        "        B = s.size(0)\n",
        "        q_pred = self.online(s)                          # [B, A, Nq]\n",
        "        q_pred_a = q_pred.gather(1, a.view(B,1,1).expand(-1,-1,self.n_quantiles)).squeeze(1)  # [B, Nq]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # action selection: mean of quantiles from online net (double DQN)\n",
        "            q_next_online = self.online(ns).mean(dim=2)      # [B, A]\n",
        "            a_star = torch.argmax(q_next_online, dim=1)      # [B]\n",
        "            q_next_tgt = self.target(ns)                     # [B, A, Nq]\n",
        "            q_next_star = q_next_tgt[torch.arange(B), a_star]# [B, Nq]\n",
        "            y = r.unsqueeze(1) + (1.0 - d.unsqueeze(1)) * self.gamma * q_next_star  # [B, Nq]\n",
        "            # reduce target across Nq by taking each column as separate target sample\n",
        "            # we will compute pairwise loss by broadcasting each target column across pred quantiles\n",
        "\n",
        "        # compute loss as mean over target columns\n",
        "        loss = 0.0\n",
        "        for j in range(self.n_quantiles):\n",
        "            loss = loss + huber_quantile_loss(q_pred_a, y[:, j], self.taus)\n",
        "        loss = loss / self.n_quantiles\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.online.parameters(), 10.0)\n",
        "        self.opt.step()\n",
        "        return float(loss.item())\n",
        "\n",
        "    def sync_target(self):\n",
        "        self.target.load_state_dict(self.online.state_dict())\n",
        "\n",
        "# Instantiate\n",
        "state_dim = env.window\n",
        "agent = QRDQNAgent(state_dim, cfg.n_actions, n_quantiles=51, gamma=cfg.gamma, lr=1e-3)\n",
        "agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b59e779e",
      "metadata": {
        "id": "b59e779e"
      },
      "source": [
        "## 6) Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43a8190",
      "metadata": {
        "id": "e43a8190"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(env, agent, buffer, steps=15000, warmup=1000, batch_size=64, sync_every=1000, eps_start=0.2, eps_end=0.02):\n",
        "    s = env.reset()\n",
        "    eps = eps_start\n",
        "    losses = []\n",
        "    rewards = []\n",
        "    total = 0.0\n",
        "    for t in range(1, steps+1):\n",
        "        a = agent.act(s, eps=eps, mode='mean')\n",
        "        ns, r, done, _ = env.step(a)\n",
        "        buffer.push(s,a,r,ns, float(done))\n",
        "        s = ns\n",
        "        total += r\n",
        "\n",
        "        if done:\n",
        "            rewards.append(total)\n",
        "            total = 0.0\n",
        "            s = env.reset()\n",
        "\n",
        "        if len(buffer) >= warmup:\n",
        "            b = buffer.sample(batch_size)\n",
        "            loss = agent.update(b)\n",
        "            losses.append(loss)\n",
        "            # linear epsilon decay\n",
        "            eps = max(eps_end, eps - (eps_start - eps_end)/ (steps - warmup + 1))\n",
        "\n",
        "        if t % sync_every == 0:\n",
        "            agent.sync_target()\n",
        "\n",
        "    return losses, rewards\n",
        "\n",
        "losses, episodic_rewards = train(env, agent, buffer, steps=8000, warmup=800, batch_size=64, sync_every=1000)\n",
        "len(losses), len(episodic_rewards)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8523df",
      "metadata": {
        "id": "0a8523df"
      },
      "source": [
        "## 7) Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060af570",
      "metadata": {
        "id": "060af570"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(6.0,3.5))\n",
        "if len(losses):\n",
        "    L = np.array(losses)\n",
        "    K = max(1, len(L)//200)\n",
        "    ma = np.convolve(L, np.ones(K)/K, mode='valid')\n",
        "    plt.plot(ma)\n",
        "plt.title(\"Training Loss (smoothed)\")\n",
        "plt.xlabel(\"updates\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6.0,3.5))\n",
        "if len(episodic_rewards):\n",
        "    R = np.array(episodic_rewards)\n",
        "    K = max(1, len(R)//50)\n",
        "    ma = np.convolve(R, np.ones(K)/K, mode='valid')\n",
        "    plt.plot(ma)\n",
        "plt.title(\"Episodic Return (smoothed)\")\n",
        "plt.xlabel(\"episodes\")\n",
        "plt.ylabel(\"return\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7aaa892",
      "metadata": {
        "id": "d7aaa892"
      },
      "source": [
        "## 8) Evaluation: Mean vs Quantile Decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5c38af0",
      "metadata": {
        "id": "b5c38af0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(env, agent, mode='mean', episodes=10):\n",
        "    totals = []\n",
        "    for _ in range(episodes):\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        total = 0.0\n",
        "        while not done:\n",
        "            a = agent.act(s, eps=0.0, mode=mode)\n",
        "            s, r, done, _ = env.step(a)\n",
        "            total += r\n",
        "        totals.append(total)\n",
        "    return np.mean(totals), np.std(totals)\n",
        "\n",
        "mean_avg, mean_std = evaluate(env, agent, mode='mean', episodes=20)\n",
        "med_avg, med_std  = evaluate(env, agent, mode=0.5,  episodes=20)   # median\n",
        "q75_avg, q75_std  = evaluate(env, agent, mode=0.75, episodes=20)   # 75th quantile\n",
        "\n",
        "print(\"Mean-based   : avg=%.3f  std=%.3f\" % (mean_avg, mean_std))\n",
        "print(\"Median-based : avg=%.3f  std=%.3f\" % (med_avg,  med_std))\n",
        "print(\"Q75-based    : avg=%.3f  std=%.3f\" % (q75_avg,  q75_std))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28799406",
      "metadata": {
        "id": "28799406"
      },
      "source": [
        "## 9) Visualize Learned Quantiles per Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ff7655",
      "metadata": {
        "id": "70ff7655"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Take random batch of states and average predicted quantiles per action\n",
        "S = []\n",
        "env.reset()\n",
        "for _ in range(256):\n",
        "    S.append(env._get_state())\n",
        "    env.t += 1\n",
        "S = torch.tensor(np.stack(S), dtype=torch.float32, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    Q = agent.online(S)  # [B, A, Nq]\n",
        "    q_mean = Q.mean(dim=0)  # [A, Nq]\n",
        "\n",
        "taus = agent.taus.detach().cpu().numpy()\n",
        "plt.figure(figsize=(7.0,4.0))\n",
        "for a in range(cfg.n_actions):\n",
        "    plt.plot(taus, q_mean[a].detach().cpu().numpy(), label=f\"action {a}\")\n",
        "plt.title(\"Predicted Quantile Functions by Action (avg over states)\")\n",
        "plt.xlabel(\"tau\")\n",
        "plt.ylabel(\"quantile value\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}