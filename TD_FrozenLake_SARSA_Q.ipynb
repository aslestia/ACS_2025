{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# TD Learning on FrozenLake (SARSA & Q-learning)\n", "\n", "Notebook ini menyajikan implementasi **Temporal Difference (TD) Learning** pada lingkungan **FrozenLake-v1** dari Gymnasium:\n", "\n", "- **SARSA (on-policy)**\n", "- **Q-learning (off-policy)**\n", "\n", "Tujuan: memahami *alur kode* (banyak komentar) dan melihat perbedaan gaya belajar keduanya.\n", "\n", "> **Catatan:** Jalankan sel-sel *berurutan dari atas ke bawah*. Jika belum memasang Gymnasium, jalankan `pip install` di sel Setup terlebih dahulu."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0) Setup & Imports\n", "Jika Gymnasium belum terpasang di lingkungan Anda, jalankan sel `pip install`.\n", "Anda bisa mengabaikan sel `pip` jika sudah terpasang."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Jika diperlukan, buka komentar baris berikut untuk memasang dependensi:\n", "# !pip install gymnasium==0.29.1 numpy matplotlib\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import random\n", "import gymnasium as gym"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Environment: FrozenLake\n", "FrozenLake adalah grid 4x4 berisi state START (S), FROZEN (F), HOLE (H), dan GOAL (G).\n", "Agen ingin mencapai GOAL tanpa jatuh ke HOLE. Pada mode `is_slippery=True`, aksi bersifat stokastik (*licin*)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n", "n_states = env.observation_space.n\n", "n_actions = env.action_space.n\n", "n_states, n_actions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Utilitas Umum\n", "- **epsilon-greedy**: eksplorasi vs eksploitasi.\n", "- **evaluate_policy**: mengukur sukses rate policy greedy terhadap Q.\n", "- **rolling_mean**: untuk menghaluskan kurva hasil."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def epsilon_greedy(Q, s, epsilon):\n", "    \"\"\"Pilih aksi dengan epsilon-greedy dari tabel Q.\n", "    - Dengan probabilitas epsilon: pilih aksi acak (eksplorasi)\n", "    - Selainnya: pilih aksi argmax Q (eksploitasi)\n", "    \"\"\"\n", "    if random.random() < epsilon:\n", "        return random.randrange(n_actions)\n", "    else:\n", "        return int(np.argmax(Q[s]))\n", "\n", "def evaluate_policy(Q, episodes=500):\n", "    \"\"\"Evaluasi policy greedy(Q): kembalikan rata-rata reward (success rate).\"\"\"\n", "    wins = 0\n", "    for _ in range(episodes):\n", "        s, _ = env.reset()\n", "        done = False\n", "        while not done:\n", "            a = int(np.argmax(Q[s]))\n", "            s, r, terminated, truncated, _ = env.step(a)\n", "            done = terminated or truncated\n", "            if done and r > 0:\n", "                wins += 1\n", "    return wins / episodes\n", "\n", "def rolling_mean(x, w=50):\n", "    if len(x) < w:\n", "        return np.array(x, dtype=float)\n", "    c = np.cumsum(np.insert(x, 0, 0))\n", "    return (c[w:] - c[:-w]) / float(w)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) SARSA (on-policy)\n", "**Rumus update:**\n", "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha\\,[r_{t+1} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)]$$\n", "\n", "- Update memakai **aksi berikutnya yang benar-benar diambil** (on-policy).\n", "- Cocok di lingkungan berisiko/acak; cenderung lebih konservatif."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_sarsa(\n", "    num_episodes=5000,\n", "    alpha=0.1,\n", "    gamma=0.99,\n", "    eps_start=1.0,\n", "    eps_end=0.05,\n", "):\n", "    Q = np.zeros((n_states, n_actions), dtype=float)\n", "    eps_history = []\n", "    perf_history = []  # success rate evaluasi berkala\n", "\n", "    for ep in range(1, num_episodes + 1):\n", "        # Linear decay epsilon\n", "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / num_episodes)\n", "        eps_history.append(epsilon)\n", "\n", "        s, _ = env.reset()\n", "        a = epsilon_greedy(Q, s, epsilon)\n", "        done = False\n", "        while not done:\n", "            s2, r, terminated, truncated, _ = env.step(a)\n", "            done = terminated or truncated\n", "            if not done:\n", "                a2 = epsilon_greedy(Q, s2, epsilon)\n", "                target = r + gamma * Q[s2, a2]\n", "            else:\n", "                target = r  # terminal\n", "            td_error = target - Q[s, a]\n", "            Q[s, a] += alpha * td_error\n", "            s, a = s2, (epsilon_greedy(Q, s2, epsilon) if not done else 0)\n", "\n", "        # evaluasi berkala (setiap 200 ep)\n", "        if ep % 200 == 0:\n", "            perf = evaluate_policy(Q, episodes=300)\n", "            perf_history.append(perf)\n", "\n", "    return Q, eps_history, perf_history\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Q-learning (off-policy)\n", "**Rumus update:**\n", "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha\\,[r_{t+1} + \\gamma \\max_a Q(s_{t+1},a) - Q(s_t,a_t)]$$\n", "\n", "- Target memakai **aksi optimal** di state berikutnya (off-policy).\n", "- Cenderung lebih agresif mengejar optimal; populer karena sederhana dan kuat."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_q_learning(\n", "    num_episodes=5000,\n", "    alpha=0.1,\n", "    gamma=0.99,\n", "    eps_start=1.0,\n", "    eps_end=0.05,\n", "):\n", "    Q = np.zeros((n_states, n_actions), dtype=float)\n", "    eps_history = []\n", "    perf_history = []\n", "\n", "    for ep in range(1, num_episodes + 1):\n", "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / num_episodes)\n", "        eps_history.append(epsilon)\n", "\n", "        s, _ = env.reset()\n", "        done = False\n", "        while not done:\n", "            a = epsilon_greedy(Q, s, epsilon)\n", "            s2, r, terminated, truncated, _ = env.step(a)\n", "            done = terminated or truncated\n", "            best_next = 0 if done else np.max(Q[s2])\n", "            target = r + gamma * best_next\n", "            td_error = target - Q[s, a]\n", "            Q[s, a] += alpha * td_error\n", "            s = s2\n", "\n", "        if ep % 200 == 0:\n", "            perf = evaluate_policy(Q, episodes=300)\n", "            perf_history.append(perf)\n", "\n", "    return Q, eps_history, perf_history\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Latih & Bandingkan\n", "Kita melatih kedua algoritma dengan hyperparameter sama (bisa diubah)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["EPISODES = 10000\n", "ALPHA = 0.1\n", "GAMMA = 0.99\n", "EPS_START = 1.0\n", "EPS_END = 0.05\n", "\n", "Q_sarsa, eps_sarsa, perf_sarsa = train_sarsa(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n", "Q_q, eps_q, perf_q = train_q_learning(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n", "\n", "avg_sarsa = evaluate_policy(Q_sarsa, episodes=1000)\n", "avg_q = evaluate_policy(Q_q, episodes=1000)\n", "avg_sarsa, avg_q"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Plot Kinerja (Success Rate)\n", "Kita plot success rate hasil evaluasi berkala (setiap 200 episode)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = np.arange(200, EPISODES+1, 200)\n", "plt.figure()\n", "plt.plot(x, perf_sarsa, marker='o', label='SARSA')\n", "plt.plot(x, perf_q, marker='s', label='Q-learning')\n", "plt.xlabel('Episodes')\n", "plt.ylabel('Success rate (eval 300 eps)')\n", "plt.title('FrozenLake TD Control: SARSA vs Q-learning')\n", "plt.legend(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Ringkasan Perbandingan\n", "- **SARSA**: on-policy \u2192 target memakai aksi yang benar-benar diambil; sering lebih konservatif.\n", "- **Q-learning**: off-policy \u2192 target pakai aksi terbaik (\\(\\max\\)); cenderung agresif.\n", "- Performa akhir sering mirip; pada lingkungan licin, SARSA bisa lebih stabil, Q-learning bisa mengejar optimal lebih cepat namun fluktuatif.\n", "\n", "Silakan ubah *hyperparameter* (episodes, alpha, gamma, epsilon schedule) untuk mengeksplorasi."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}