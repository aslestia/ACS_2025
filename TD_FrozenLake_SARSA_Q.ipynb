{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslestia/ACS_2025/blob/main/TD_FrozenLake_SARSA_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScyOtvcyuIOu"
      },
      "source": [
        "# TD Learning on FrozenLake (SARSA & Q-learning)\n",
        "\n",
        "Notebook ini menyajikan implementasi **Temporal Difference (TD) Learning** pada lingkungan **FrozenLake-v1** dari Gymnasium:\n",
        "\n",
        "- **SARSA (on-policy)**\n",
        "- **Q-learning (off-policy)**\n",
        "\n",
        "Tujuan: memahami *alur kode* (banyak komentar) dan melihat perbedaan gaya belajar keduanya.\n",
        "\n",
        "> **Catatan:** Jalankan sel-sel *berurutan dari atas ke bawah*. Jika belum memasang Gymnasium, jalankan `pip install` di sel Setup terlebih dahulu."
      ],
      "id": "ScyOtvcyuIOu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPAxYBTiuIOx"
      },
      "source": [
        "## 0) Setup & Imports\n",
        "Jika Gymnasium belum terpasang di lingkungan Anda, jalankan sel `pip install`.\n",
        "Anda bisa mengabaikan sel `pip` jika sudah terpasang."
      ],
      "id": "RPAxYBTiuIOx"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JxpVGyl3uIOy"
      },
      "outputs": [],
      "source": [
        "# Jika diperlukan, buka komentar baris berikut untuk memasang dependensi:\n",
        "# !pip install gymnasium==0.29.1 numpy matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gymnasium as gym"
      ],
      "id": "JxpVGyl3uIOy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg6xwXsjuIOz"
      },
      "source": [
        "## 1) Environment: FrozenLake\n",
        "FrozenLake adalah grid 4x4 berisi state START (S), FROZEN (F), HOLE (H), dan GOAL (G).\n",
        "Agen ingin mencapai GOAL tanpa jatuh ke HOLE. Pada mode `is_slippery=True`, aksi bersifat stokastik (*licin*)."
      ],
      "id": "Jg6xwXsjuIOz"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h6cf2x4IuIO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e52f306-c015-448d-8d0a-af8bb245e28e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.int64(16), np.int64(4))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "n_states, n_actions"
      ],
      "id": "h6cf2x4IuIO0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MWrUUlTuIO0"
      },
      "source": [
        "## 2) Utilitas Umum\n",
        "- **epsilon-greedy**: eksplorasi vs eksploitasi.\n",
        "- **evaluate_policy**: mengukur sukses rate policy greedy terhadap Q.\n",
        "- **rolling_mean**: untuk menghaluskan kurva hasil."
      ],
      "id": "1MWrUUlTuIO0"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QVvnjbc4uIO0"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(Q, s, epsilon):\n",
        "    \"\"\"Pilih aksi dengan epsilon-greedy dari tabel Q.\n",
        "    - Dengan probabilitas epsilon: pilih aksi acak (eksplorasi)\n",
        "    - Selainnya: pilih aksi argmax Q (eksploitasi)\n",
        "    \"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return random.randrange(n_actions)\n",
        "    else:\n",
        "        return int(np.argmax(Q[s]))\n",
        "\n",
        "def evaluate_policy(Q, episodes=500):\n",
        "    \"\"\"Evaluasi policy greedy(Q): kembalikan rata-rata reward (success rate).\"\"\"\n",
        "    wins = 0\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = int(np.argmax(Q[s]))\n",
        "            s, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            if done and r > 0:\n",
        "                wins += 1\n",
        "    return wins / episodes\n",
        "\n",
        "def rolling_mean(x, w=50):\n",
        "    if len(x) < w:\n",
        "        return np.array(x, dtype=float)\n",
        "    c = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (c[w:] - c[:-w]) / float(w)"
      ],
      "id": "QVvnjbc4uIO0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtJZZS0QuIO1"
      },
      "source": [
        "## 3) SARSA (on-policy)\n",
        "**Rumus update:**\n",
        "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha\\,[r_{t+1} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)]$$\n",
        "\n",
        "- Update memakai **aksi berikutnya yang benar-benar diambil** (on-policy).\n",
        "- Cocok di lingkungan berisiko/acak; cenderung lebih konservatif."
      ],
      "id": "DtJZZS0QuIO1"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "avD3C6AQuIO1"
      },
      "outputs": [],
      "source": [
        "def train_sarsa(\n",
        "    num_episodes=5000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    eps_start=1.0,\n",
        "    eps_end=0.05,\n",
        "):\n",
        "    Q = np.zeros((n_states, n_actions), dtype=float)\n",
        "    eps_history = []\n",
        "    perf_history = []  # success rate evaluasi berkala\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        # Linear decay epsilon\n",
        "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / num_episodes)\n",
        "        eps_history.append(epsilon)\n",
        "\n",
        "        s, _ = env.reset()\n",
        "        a = epsilon_greedy(Q, s, epsilon)\n",
        "        done = False\n",
        "        while not done:\n",
        "            s2, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            if not done:\n",
        "                a2 = epsilon_greedy(Q, s2, epsilon)\n",
        "                target = r + gamma * Q[s2, a2]\n",
        "            else:\n",
        "                target = r  # terminal\n",
        "            td_error = target - Q[s, a]\n",
        "            Q[s, a] += alpha * td_error\n",
        "            s, a = s2, (epsilon_greedy(Q, s2, epsilon) if not done else 0)\n",
        "\n",
        "        # evaluasi berkala (setiap 200 ep)\n",
        "        if ep % 200 == 0:\n",
        "            perf = evaluate_policy(Q, episodes=300)\n",
        "            perf_history.append(perf)\n",
        "\n",
        "    return Q, eps_history, perf_history\n"
      ],
      "id": "avD3C6AQuIO1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4dhiWljuIO1"
      },
      "source": [
        "## 4) Q-learning (off-policy)\n",
        "**Rumus update:**\n",
        "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha\\,[r_{t+1} + \\gamma \\max_a Q(s_{t+1},a) - Q(s_t,a_t)]$$\n",
        "\n",
        "- Target memakai **aksi optimal** di state berikutnya (off-policy).\n",
        "- Cenderung lebih agresif mengejar optimal; populer karena sederhana dan kuat."
      ],
      "id": "z4dhiWljuIO1"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YK__cm5uuIO2"
      },
      "outputs": [],
      "source": [
        "def train_q_learning(\n",
        "    num_episodes=5000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    eps_start=1.0,\n",
        "    eps_end=0.05,\n",
        "):\n",
        "    Q = np.zeros((n_states, n_actions), dtype=float)\n",
        "    eps_history = []\n",
        "    perf_history = []\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        epsilon = max(eps_end, eps_start - (eps_start - eps_end) * ep / num_episodes)\n",
        "        eps_history.append(epsilon)\n",
        "\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = epsilon_greedy(Q, s, epsilon)\n",
        "            s2, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            best_next = 0 if done else np.max(Q[s2])\n",
        "            target = r + gamma * best_next\n",
        "            td_error = target - Q[s, a]\n",
        "            Q[s, a] += alpha * td_error\n",
        "            s = s2\n",
        "\n",
        "        if ep % 200 == 0:\n",
        "            perf = evaluate_policy(Q, episodes=300)\n",
        "            perf_history.append(perf)\n",
        "\n",
        "    return Q, eps_history, perf_history\n"
      ],
      "id": "YK__cm5uuIO2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdUvygw9uIO2"
      },
      "source": [
        "## 5) Latih & Bandingkan\n",
        "Kita melatih kedua algoritma dengan hyperparameter sama (bisa diubah)."
      ],
      "id": "SdUvygw9uIO2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VCPDmLGuIO2"
      },
      "outputs": [],
      "source": [
        "EPISODES = 10000\n",
        "ALPHA = 0.1\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "\n",
        "Q_sarsa, eps_sarsa, perf_sarsa = train_sarsa(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n",
        "Q_q, eps_q, perf_q = train_q_learning(EPISODES, ALPHA, GAMMA, EPS_START, EPS_END)\n",
        "\n",
        "avg_sarsa = evaluate_policy(Q_sarsa, episodes=1000)\n",
        "avg_q = evaluate_policy(Q_q, episodes=1000)\n",
        "avg_sarsa, avg_q"
      ],
      "id": "3VCPDmLGuIO2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B72QPvKJuIO2"
      },
      "source": [
        "## 6) Plot Kinerja (Success Rate)\n",
        "Kita plot success rate hasil evaluasi berkala (setiap 200 episode)."
      ],
      "id": "B72QPvKJuIO2"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xc2xagkevSCS"
      },
      "id": "Xc2xagkevSCS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v27E3l8huIO3"
      },
      "source": [
        "## 7) Ringkasan Perbandingan\n",
        "- **SARSA**: on-policy → target memakai aksi yang benar-benar diambil; sering lebih konservatif.\n",
        "- **Q-learning**: off-policy → target pakai aksi terbaik (\\(\\max\\)); cenderung agresif.\n",
        "- Performa akhir sering mirip; pada lingkungan licin, SARSA bisa lebih stabil, Q-learning bisa mengejar optimal lebih cepat namun fluktuatif.\n",
        "\n",
        "**Silakan ubah *hyperparameter* (episodes, alpha, gamma, epsilon schedule) untuk mengeksplorasi.**"
      ],
      "id": "v27E3l8huIO3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}