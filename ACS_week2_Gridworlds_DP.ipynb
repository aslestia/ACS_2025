{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZlJFKp+nUh0m+/hhDDLQr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslestia/ACS_2025/blob/main/ACS_week2_Gridworlds_DP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVvUvSTmQZP5"
      },
      "outputs": [],
      "source": [
        "#Pseudocode — GridWorld lengkap (sampai DP)\n",
        "ENV:\n",
        "  sama seperti (1), bisa tambah slip probability (transisi probabilistik).\n",
        "  TRANSITIONS(s, a) → daftar (p, s', r)\n",
        "\n",
        "VALUE ITERATION:\n",
        "  Inisialisasi V(s) = 0\n",
        "  repeat:\n",
        "    for setiap state s (non-terminal):\n",
        "      V_new(s) = max_a Σ_{s'} p(s'|s,a) [ r(s,a,s') + γ V(s') ]\n",
        "    cek konvergensi ||V_new - V|| ≤ ε\n",
        "    V ← V_new\n",
        "  Ekstrak policy: π(s) = argmax_a Σ p(s'|s,a)[ r + γ V(s') ]\n",
        "\n",
        "POLICY ITERATION:\n",
        "  Inisialisasi π acak\n",
        "  repeat:\n",
        "    (Evaluasi)  hitung V^π dengan: V(s) = Σ p [ r + γ V(s') ] untuk aksi π(s)\n",
        "    (Perbaikan) π_baru(s) = argmax_a Σ p [ r + γ V^π(s') ]\n",
        "  until π_baru = π"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code TODO — GridWorld lengkap (tugas)\n",
        "#Isi baris bertanda # TODO.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "ACTIONS = {0:(-1,0), 1:(0,1), 2:(1,0), 3:(0,-1)}   # U,R,D,L\n",
        "ARROWS  = {0:\"↑\", 1:\"→\", 2:\"↓\", 3:\"←\", -1:\"■\"}\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, H=4, W=4, start=(0,0), goals={(3,3):+10}, traps={(1,2):-10,(2,1):-10},\n",
        "                 step_reward=-1.0, gamma=0.95, slip=0.0):\n",
        "        self.H,self.W = H,W; self.start=start\n",
        "        self.goals, self.traps = goals, traps\n",
        "        self.step_reward = step_reward; self.gamma = gamma; self.slip = slip\n",
        "        self.nS, self.nA = H*W, 4\n",
        "\n",
        "    def idx(self,s): return s[0]*self.W + s[1]\n",
        "    def state_from_idx(self,i): return (i//self.W, i%self.W)\n",
        "    def in_bounds(self,r,c): return 0<=r<self.H and 0<=c<self.W\n",
        "    def is_terminal(self,s): return s in self.goals or s in self.traps\n",
        "\n",
        "    def transitions(self, s, a):\n",
        "        \"\"\"Kembalikan list (p, s_next, reward).\"\"\"\n",
        "        if self.is_terminal(s):\n",
        "            return [(1.0, s, 0.0)]\n",
        "        dr,dc = ACTIONS[a]\n",
        "        cand = (s[0]+dr, s[1]+dc)\n",
        "        if not self.in_bounds(*cand): cand = s\n",
        "\n",
        "        left, right = (a-1)%4, (a+1)%4\n",
        "        candL = (s[0]+ACTIONS[left][0], s[1]+ACTIONS[left][1])\n",
        "        candR = (s[0]+ACTIONS[right][0], s[1]+ACTIONS[right][1])\n",
        "        candL = candL if self.in_bounds(*candL) else s\n",
        "        candR = candR if self.in_bounds(*candR) else s\n",
        "\n",
        "        p_main = 1.0 - 2*self.slip\n",
        "        outcomes = [(p_main,cand),(self.slip,candL),(self.slip,candR)]\n",
        "\n",
        "        ts=[]\n",
        "        for p,sn in outcomes:\n",
        "            if p<=0: continue\n",
        "            r = self.goals.get(sn, self.traps.get(sn, self.step_reward))\n",
        "            ts.append((p,sn,r))\n",
        "        return ts\n",
        "\n",
        "def print_values(env, V):\n",
        "    grid = np.zeros((env.H, env.W))\n",
        "    for i in range(env.nS):\n",
        "        r,c = env.state_from_idx(i); grid[r,c] = V[i]\n",
        "    for r in range(env.H):\n",
        "        print(\" | \".join(f\"{grid[r,c]:4.1f}\" for c in range(env.W)))\n",
        "    print()\n",
        "\n",
        "def print_policy(env, Pi):\n",
        "    for r in range(env.H):\n",
        "        row=[]\n",
        "        for c in range(env.W):\n",
        "            s=(r,c)\n",
        "            row.append(ARROWS[-1] if env.is_terminal(s) else ARROWS[Pi[env.idx(s)]])\n",
        "        print(\" \".join(row))\n",
        "    print()\n",
        "\n",
        "# ---------- TODO 1: Value Iteration ----------\n",
        "def value_iteration(env, eps=1e-6, max_iter=1000):\n",
        "    V = np.zeros(env.nS)\n",
        "    iters=0\n",
        "    for it in range(max_iter):\n",
        "        iters=it+1; delta=0.0\n",
        "        V_new = V.copy()\n",
        "        for i in range(env.nS):\n",
        "            s = env.state_from_idx(i)\n",
        "            if env.is_terminal(s):\n",
        "                V_new[i] = 0.0\n",
        "                continue\n",
        "            # TODO: hitung nilai Bellman optimal untuk state s\n",
        "            qbest = -1e18\n",
        "            for a in range(env.nA):\n",
        "                q = 0.0\n",
        "                for p,sn,r in env.transitions(s,a):\n",
        "                    q += p * ( r + env.gamma * V[ env.idx(sn) ] )\n",
        "                qbest = max(qbest, q)\n",
        "            V_new[i] = qbest\n",
        "            delta = max(delta, abs(V_new[i]-V[i]))\n",
        "        V = V_new\n",
        "        if delta < eps: break\n",
        "\n",
        "    # TODO: ekstrak policy greedy dari V\n",
        "    Pi = np.full(env.nS, -1, int)\n",
        "    for i in range(env.nS):\n",
        "        s = env.state_from_idx(i)\n",
        "        if env.is_terminal(s): continue\n",
        "        best_a, best_q = 0, -1e18\n",
        "        for a in range(env.nA):\n",
        "            q=0.0\n",
        "            for p,sn,r in env.transitions(s,a):\n",
        "                q += p * ( r + env.gamma * V[env.idx(sn)] )\n",
        "            if q>best_q: best_q, best_a = q, a\n",
        "        Pi[i] = best_a\n",
        "    return V, Pi, iters\n",
        "\n",
        "# ---------- TODO 2: Policy Evaluation ----------\n",
        "def policy_evaluation(env, Pi, eps=1e-6, max_iter=1000):\n",
        "    V = np.zeros(env.nS)\n",
        "    for _ in range(max_iter):\n",
        "        delta=0.0\n",
        "        for i in range(env.nS):\n",
        "            s = env.state_from_idx(i)\n",
        "            if env.is_terminal(s): newv = 0.0\n",
        "            else:\n",
        "                a = Pi[i]; newv = 0.0\n",
        "                # TODO: evaluasi nilai mengikuti aksi a\n",
        "                for p,sn,r in env.transitions(s,a):\n",
        "                    newv += p * ( r + env.gamma * V[ env.idx(sn) ] )\n",
        "            delta = max(delta, abs(newv - V[i]))\n",
        "            V[i] = newv\n",
        "        if delta < eps: break\n",
        "    return V\n",
        "\n",
        "# ---------- TODO 3: Policy Improvement ----------\n",
        "def policy_improvement(env, V):\n",
        "    Pi = np.full(env.nS, -1, int)\n",
        "    for i in range(env.nS):\n",
        "        s = env.state_from_idx(i)\n",
        "        if env.is_terminal(s): continue\n",
        "        # TODO: pilih aksi terbaik berdasar V\n",
        "        best_a, best_q = 0, -1e18\n",
        "        for a in range(env.nA):\n",
        "            q=0.0\n",
        "            for p,sn,r in env.transitions(s,a):\n",
        "                q += p * ( r + env.gamma * V[ env.idx(sn) ] )\n",
        "            if q>best_q: best_q, best_a = q, a\n",
        "        Pi[i] = best_a\n",
        "    return Pi\n",
        "\n",
        "def policy_iteration(env, eps=1e-6, max_iter=100):\n",
        "    # inisialisasi policy (misal semua 'Up') kecuali terminal\n",
        "    Pi = np.zeros(env.nS, dtype=int)\n",
        "    for i in range(env.nS):\n",
        "        if env.is_terminal(env.state_from_idx(i)): Pi[i] = -1\n",
        "    iters=0\n",
        "    for k in range(max_iter):\n",
        "        iters=k+1\n",
        "        V = policy_evaluation(env, Pi, eps=eps)\n",
        "        newPi = policy_improvement(env, V)\n",
        "        if np.array_equal(newPi, Pi): return V, Pi, iters\n",
        "        Pi = newPi\n",
        "    return V, Pi, iters\n",
        "\n",
        "# ---------- Blok eksperimen (tinggal jalankan) ----------\n",
        "env = GridWorld()\n",
        "V_vi, Pi_vi, it_vi = value_iteration(env)\n",
        "print(\"Value Iteration — Values:\"); print_values(env, V_vi)\n",
        "print(\"Value Iteration — Policy:\"); print_policy(env, Pi_vi)\n",
        "print(\"Iterasi:\", it_vi)\n",
        "\n",
        "V_pi, Pi_pi, it_pi = policy_iteration(env)\n",
        "print(\"Policy Iteration — Values:\"); print_values(env, V_pi)\n",
        "print(\"Policy Iteration — Policy:\"); print_policy(env, Pi_pi)\n",
        "print(\"Iterasi:\", it_pi)"
      ],
      "metadata": {
        "id": "al_G9_YdQfC8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}